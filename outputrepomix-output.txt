This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: .venv/, *.csv, *.json, uv.lock
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
src/
  api/
    models/
      __init__.py
      citations.py
      converters.py
      opinions.py
      pipeline.py
      stats.py
    routers/
      __init__.py
      citations.py
      opinions.py
      pipeline.py
      stats.py
    services/
      __init__.py
      citation_service.py
      db_utils.py
      opinion_service.py
      pipeline_service.py
      stats_service.py
    __init__.py
    database.py
    main.py
  llm_extraction/
    models.py
    prompts.py
    rate_limited_gemini.py
  neo4j/
    models.py
    neomodel_loader.py
  neo4j_import/
    helper_scripts/
      query_build_citation_map.sh
      query_build_opinion_cluster.py
    citations.header
    import_opinions.sh
    opinions.header
  postgres/
    database.py
    models.py
.dockerignore
.env.prod
.gitignore
.python-version
cline_planning.md
docker-compose.yml
docker-entrypoint.sh
Dockerfile
pyproject.toml
query.sh
README.md

================================================================
Files
================================================================

================
File: src/api/models/__init__.py
================
"""
API model definitions for the US Courts Legal Explorer API.

This package contains Pydantic models used for:
1. API request/response validation
2. Documentation generation
3. Data conversion between internal models and API responses

Note: Many models now directly use or extend core models from src.llm_extraction.models,
src.neo4j.models, and src.postgres.models to reduce duplication.
"""

from . import opinions, citations, stats, pipeline, converters

# Export all modules
__all__ = ["opinions", "citations", "stats", "pipeline", "converters"]

================
File: src/api/models/citations.py
================
"""
This module has been refactored to remove duplicate citation model definitions.
All core citation-related models are now imported from src/llm_extraction.models.

You should now use the core models for citation information.

Note: If additional API-specific extensions are required, they can be added here.
"""

from src.llm_extraction.models import Citation, CitationAnalysis, CitationResolved

__all__ = ['Citation', 'CitationAnalysis', 'CitationResolved']

================
File: src/api/models/converters.py
================
'''
Converters for API models.

This module provides converters for transforming between different model representations:
- Neo4j models (neomodel)
- PostgreSQL models (SQLAlchemy)
- API models (Pydantic)

This has been refactored to focus only on essential conversions that aren't handled
by direct model usage or inheritance.
'''

from datetime import datetime

# Import models from source of truth
from src.neo4j.models import Opinion as Neo4jOpinion
from src.postgres.models import OpinionClusterExtraction, CitationExtraction
from src.llm_extraction.models import OpinionSection, CitationType, CitationTreatment

def neo4j_opinion_to_core(opinion: Neo4jOpinion) -> dict:
    """
    Convert a Neo4j Opinion node to a core dictionary representation.
    
    This is a utility function for cases where direct model instantiation
    isn't possible or practical.
    
    Args:
        opinion: Neo4j Opinion node
        
    Returns:
        Dictionary with core opinion data
    """
    return {
        "cluster_id": opinion.cluster_id,
        "case_name": opinion.case_name,
        "date_filed": opinion.date_filed.isoformat() if opinion.date_filed else None,
        "docket_id": opinion.docket_id,
        "docket_number": opinion.docket_number,
        "court_id": opinion.court_id,
        "court_name": opinion.court_name,
        "opinion_type": opinion.opinion_type,
        "scdb_votes_majority": opinion.scdb_votes_majority,
        "scdb_votes_minority": opinion.scdb_votes_minority,
        "ai_summary": opinion.ai_summary,
    }

def postgres_citation_to_dict(citation: CitationExtraction) -> dict:
    """
    Convert a PostgreSQL CitationExtraction to a dictionary representation.
    
    Args:
        citation: SQLAlchemy CitationExtraction model
        
    Returns:
        Dictionary with citation data
    """
    return {
        "id": citation.id,
        "opinion_cluster_extraction_id": citation.opinion_cluster_extraction_id,
        "section": citation.section,
        "citation_type": citation.citation_type,
        "citation_text": citation.citation_text,
        "page_number": citation.page_number,
        "treatment": citation.treatment,
        "relevance": citation.relevance,
        "reasoning": citation.reasoning,
        "resolved_opinion_cluster": citation.resolved_opinion_cluster,
        "resolved_text": citation.resolved_text,
        "created_at": citation.created_at.isoformat() if citation.created_at else None,
        "updated_at": citation.updated_at.isoformat() if citation.updated_at else None,
    }

# Note: Many previous converters are no longer needed because models now directly
# use or extend the source-of-truth models and have their own from_* conversion methods.

================
File: src/api/models/opinions.py
================
"""
API models for opinions.

This module refactors API models to use source-of-truth models from:
- src.llm_extraction.models
- src.neo4j.models 
- src.postgres.models

It uses them directly where possible and extends them where API-specific needs exist.
"""
from pydantic import BaseModel, Field, ConfigDict
from typing import List, Optional
from datetime import date

from src.neo4j.models import Opinion as Neo4jOpinion
from src.llm_extraction.models import OpinionSection, CitationTreatment, Citation, OpinionType
from src.api.models.converters import neo4j_opinion_to_core

# Re-export models from source of truth for API use
__all__ = ['Citation', 'OpinionSection', 'CitationTreatment', 'OpinionType', 'OpinionBase', 'OpinionResponse', 'OpinionDetail', 'OpinionText', 'OpinionCitation']

class OpinionBase(BaseModel):
    """Base model for opinion data."""
    cluster_id: int = Field(..., description="Unique identifier for the opinion cluster")
    case_name: str = Field(..., description="Name of the case")
    date_filed: date = Field(..., description="Date the opinion was filed")
    
    @classmethod
    def from_neo4j(cls, opinion: Neo4jOpinion):
        """Create an OpinionBase from a Neo4j Opinion model."""
        return cls(
            cluster_id=opinion.cluster_id,
            case_name=opinion.case_name,
            date_filed=opinion.date_filed  # use the original date object
        )
    
    model_config = ConfigDict(
        from_attributes=True  # Replaces deprecated orm_mode=True
    )
    
class OpinionResponse(OpinionBase):
    """Model for opinion summary response."""
    court_id: str = Field(..., description="Court identifier")
    court_name: str = Field(..., description="Name of the court")
    docket_number: Optional[str] = Field(None, description="Docket number")
    citation_count: int = Field(0, description="Number of times this opinion has been cited")
    
    @classmethod
    def from_neo4j(cls, opinion: Neo4jOpinion):
        """Create an OpinionResponse from a Neo4j Opinion model."""
        base = OpinionBase.from_neo4j(opinion)
        return cls(
            **base.model_dump(),
            court_id=str(opinion.court_id),  # Ensure court_id is a string
            court_name=opinion.court_name or 'Unknown Court',
            docket_number=opinion.docket_number,
            citation_count=len(opinion.cited_by.all()) if hasattr(opinion, 'cited_by') else 0
        )

class OpinionDetail(OpinionResponse):
    """Model for detailed opinion information."""
    docket_id: Optional[int] = Field(None, description="Docket identifier")
    scdb_votes_majority: Optional[int] = Field(None, description="Supreme Court Database majority votes")
    scdb_votes_minority: Optional[int] = Field(None, description="Supreme Court Database minority votes")
    brief_summary: Optional[str] = Field(None, description="Brief summary of the opinion")
    opinion_type: Optional[OpinionType] = Field(None, description="Type of opinion document")
    
    @classmethod
    def from_neo4j(cls, opinion: Neo4jOpinion):
        """Create an OpinionDetail from a Neo4j Opinion model."""
        base = OpinionResponse.from_neo4j(opinion)
        return cls(
            **base.model_dump(),
            docket_id=opinion.docket_id,
            scdb_votes_majority=opinion.scdb_votes_majority,
            scdb_votes_minority=opinion.scdb_votes_minority,
            brief_summary=opinion.ai_summary,
            opinion_type=opinion.opinion_type
        )

class OpinionText(BaseModel):
    """Model for opinion text."""
    cluster_id: int = Field(..., description="Opinion cluster identifier")
    text: str = Field(..., description="Full text of the opinion")
    
    model_config = ConfigDict(
        from_attributes=True  # Replaces deprecated orm_mode=True
    )

class OpinionCitation(BaseModel):
    """Model for citation information within an opinion."""
    citing_id: int = Field(..., description="Citing opinion cluster identifier")
    cited_id: int = Field(..., description="Cited opinion cluster identifier")
    citation_text: str = Field(..., description="Original citation text")
    page_number: Optional[int] = Field(None, description="Page number where the citation appears")
    treatment: Optional[CitationTreatment] = Field(None, description="Citation treatment")
    relevance: Optional[int] = Field(None, description="Relevance score (1-4)")
    reasoning: Optional[str] = Field(None, description="Reasoning for the citation")
    opinion_section: Optional[OpinionSection] = Field(None, description="Section of the opinion")
    
    @classmethod
    def from_neo4j_rel(cls, rel, citing_id: int, cited_id: int):
        """Create an OpinionCitation from a Neo4j CitesRel relationship."""
        return cls(
            citing_id=citing_id,
            cited_id=cited_id,
            citation_text=rel.citation_text or "",
            page_number=rel.page_number,
            treatment=rel.treatment,
            relevance=rel.relevance,
            reasoning=rel.reasoning,
            opinion_section=rel.opinion_section
        )
    
    model_config = ConfigDict(
        from_attributes=True  # Replaces deprecated orm_mode=True
    )

================
File: src/api/models/pipeline.py
================
"""
API models for pipeline operations.

This module refactors API models to use source-of-truth models from:
- src.llm_extraction.models
- src.neo4j.models 
- src.postgres.models

It uses them directly where possible and extends them where API-specific needs exist.
"""
from pydantic import BaseModel, Field, validator, ConfigDict
from typing import List, Dict, Optional, Any, Union
from datetime import datetime, date
from enum import Enum

# Import source-of-truth models
from src.llm_extraction.models import (
    CitationAnalysis, 
    CombinedResolvedCitationAnalysis, 
    Citation, 
    CitationResolved
)
from src.neo4j.neomodel_loader import NeomodelLoader

# Re-export source-of-truth models for API use
__all__ = [
    'CitationAnalysis', 
    'CombinedResolvedCitationAnalysis', 
    'Citation', 
    'CitationResolved',
    'JobStatus',
    'JobType',
    'ExtractionConfig',
    'PipelineJob',
    'PipelineStatus',
    'LLMProcessResult',
    'ResolutionResult',
    'Neo4jLoadResult',
    'PipelineResult',
    'PipelineStats'
]

class JobStatus(str, Enum):
    """Enum for pipeline job status."""
    QUEUED = "queued"
    STARTED = "started"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class JobType(str, Enum):
    """Enum for pipeline job types."""
    EXTRACT = "extract"
    LLM_PROCESS = "llm_process"
    CITATION_RESOLUTION = "citation_resolution"
    NEO4J_LOAD = "neo4j_load"
    CSV_UPLOAD = "csv_upload"
    FULL_PIPELINE = "full_pipeline"

class ExtractionConfig(BaseModel):
    """Configuration for opinion extraction."""
    court_id: Optional[str] = Field(None, description="Filter by court ID")
    start_date: Optional[date] = Field(None, description="Filter by date range (start)")
    end_date: Optional[date] = Field(None, description="Filter by date range (end)")
    limit: Optional[int] = Field(None, description="Maximum number of opinions to extract")
    offset: Optional[int] = Field(0, description="Number of opinions to skip")
    include_text: bool = Field(True, description="Include opinion text in extraction")
    include_metadata: bool = Field(True, description="Include opinion metadata in extraction")
    
    @validator('limit')
    def validate_limit(cls, v):
        if v is not None and v <= 0:
            raise ValueError('limit must be positive')
        return v
    
    @validator('offset')
    def validate_offset(cls, v):
        if v < 0:
            raise ValueError('offset must be non-negative')
        return v

class PipelineJob(BaseModel):
    """Model for pipeline job information."""
    job_id: int = Field(..., description="Job identifier")
    status: JobStatus = Field(..., description="Job status")
    message: Optional[str] = Field(None, description="Status message")

class PipelineStatus(BaseModel):
    """Model for pipeline job status."""
    job_id: int = Field(..., description="Job identifier")
    job_type: JobType = Field(..., description="Job type")
    status: JobStatus = Field(..., description="Job status")
    created_at: datetime = Field(..., description="Job creation timestamp")
    started_at: Optional[datetime] = Field(None, description="Job start timestamp")
    completed_at: Optional[datetime] = Field(None, description="Job completion timestamp")
    config: Dict[str, Any] = Field(..., description="Job configuration")
    progress: Optional[float] = Field(None, description="Job progress (0-100)")
    message: Optional[str] = Field(None, description="Status message")
    error: Optional[str] = Field(None, description="Error message if job failed")
    result_path: Optional[str] = Field(None, description="Path to job result file")
    
    model_config = ConfigDict(
        from_attributes=True  # Replaces deprecated orm_mode=True
    )

class LLMProcessResult(BaseModel):
    """Model for LLM processing result."""
    cluster_id: int = Field(..., description="Opinion cluster identifier")
    citation_analysis: CitationAnalysis = Field(..., description="Citation analysis result")
    
    @classmethod
    def from_gemini_response(cls, cluster_id: int, response: Dict[str, Any]):
        """Create LLMProcessResult from Gemini response."""
        import json
        from json_repair import repair_json
        
        citation_data = None
        if not cluster_id:
            raise ValueError("Cluster ID is required")
        
        # Handle the response based on its structure
        if "parsed" in response and response["parsed"] is not None:
            citation_data = response["parsed"]
            
            # If citation_data is still a string, parse it
            if isinstance(citation_data, str):
                try:
                    citation_data = json.loads(citation_data)
                except json.JSONDecodeError:
                    # Try to repair malformed JSON
                    citation_data = json.loads(repair_json(citation_data))
        else:
            # Try to extract from the candidate content
            try:
                raw_text = response["candidates"][0]["content"]["parts"][0]["text"]
                repaired_json = repair_json(raw_text)
                citation_data = json.loads(repaired_json)
            except (KeyError, IndexError, json.JSONDecodeError) as e:
                raise ValueError(f"Could not extract citation data from response: {str(e)}")
        
        # Validate and create CitationAnalysis
        citation_analysis = CitationAnalysis(
            date=citation_data["date"],
            brief_summary=citation_data["brief_summary"],
            majority_opinion_citations=citation_data.get("majority_opinion_citations", []),
            concurring_opinion_citations=citation_data.get("concurring_opinion_citations", []),
            dissenting_citations=citation_data.get("dissenting_citations", [])
        )
        
        return cls(cluster_id=cluster_id, citation_analysis=citation_analysis)

class ResolutionResult(BaseModel):
    """Model for citation resolution result."""
    cluster_id: int = Field(..., description="Opinion cluster identifier")
    resolved_citations: CombinedResolvedCitationAnalysis = Field(..., description="Resolved citation analysis")
    
    @classmethod
    def from_llm_result(cls, llm_result: LLMProcessResult):
        """Create ResolutionResult from LLMProcessResult."""
        from src.llm_extraction.models import CombinedResolvedCitationAnalysis
        
        # Use the built-in method from CombinedResolvedCitationAnalysis
        resolved = CombinedResolvedCitationAnalysis.from_citations(
            [llm_result.citation_analysis], 
            llm_result.cluster_id
        )
        
        return cls(
            cluster_id=llm_result.cluster_id,
            resolved_citations=resolved
        )

class Neo4jLoadResult(BaseModel):
    """Model for Neo4j loading result."""
    cluster_ids: List[int] = Field(..., description="Loaded opinion cluster IDs")
    citation_count: int = Field(..., description="Number of citations loaded")
    
    @classmethod
    def from_loader_result(cls, loader_result: Dict[str, Any]):
        """Create Neo4jLoadResult from loader result."""
        return cls(
            cluster_ids=loader_result.get("cluster_ids", []),
            citation_count=loader_result.get("citation_count", 0)
        )

class PipelineResult(BaseModel):
    """Model for pipeline job result."""
    job_id: int = Field(..., description="Job identifier")
    job_type: JobType = Field(..., description="Job type")
    status: JobStatus = Field(..., description="Job status")
    created_at: datetime = Field(..., description="Job creation timestamp")
    completed_at: Optional[datetime] = Field(None, description="Job completion timestamp")
    result: Dict[str, Any] = Field(..., description="Job result data")
    
    model_config = ConfigDict(
        from_attributes=True  # Replaces deprecated orm_mode=True
    )

class PipelineStats(BaseModel):
    """Model for pipeline statistics."""
    total_jobs: int = Field(..., description="Total number of jobs")
    jobs_by_type: Dict[str, int] = Field(..., description="Jobs by type")
    jobs_by_status: Dict[str, int] = Field(..., description="Jobs by status")
    avg_processing_time: Dict[str, float] = Field(..., description="Average processing time by job type (seconds)")
    recent_jobs: List[PipelineStatus] = Field(..., description="Recent jobs")

================
File: src/api/models/stats.py
================
"""
API models for statistics.

This module refactors API models to use source-of-truth models from:
- src.llm_extraction.models
- src.neo4j.models 
- src.postgres.models

It uses them directly where possible and extends them where API-specific needs exist.
"""
from pydantic import BaseModel, Field, ConfigDict
from typing import List, Dict, Optional, Any
from datetime import date

from src.neo4j.models import Opinion as Neo4jOpinion

__all__ = [
    'NetworkStats',
    'CourtStats',
    'YearlyStats',
    'TimelineStats',
    'TopCitedOpinion',
    'TopCitedOpinions'
]

class NetworkStats(BaseModel):
    """Model for overall network statistics."""
    total_nodes: int = Field(..., description="Total number of nodes in the network")
    total_edges: int = Field(..., description="Total number of edges in the network")
    network_density: float = Field(..., description="Network density (ratio of actual to possible connections)")
    avg_degree: float = Field(..., description="Average node degree (number of connections)")
    max_degree: int = Field(..., description="Maximum node degree")
    avg_path_length: Optional[float] = Field(None, description="Average shortest path length")
    diameter: Optional[int] = Field(None, description="Network diameter (longest shortest path)")
    clustering_coefficient: Optional[float] = Field(None, description="Global clustering coefficient")
    connected_components: Optional[int] = Field(None, description="Number of connected components")
    largest_component_size: Optional[int] = Field(None, description="Size of the largest connected component")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Additional metadata")

class CourtStats(BaseModel):
    """Model for court-specific statistics."""
    court_id: str = Field(..., description="Court identifier")
    court_name: str = Field(..., description="Court name")
    opinion_count: int = Field(..., description="Number of opinions from this court")
    citation_count: int = Field(..., description="Number of citations from this court")
    cited_by_count: int = Field(..., description="Number of times opinions from this court are cited")
    self_citation_ratio: float = Field(..., description="Ratio of citations to the same court")
    avg_citations_per_opinion: float = Field(..., description="Average citations per opinion")
    top_cited_courts: List[Dict[str, Any]] = Field(..., description="Top courts cited by this court")
    top_citing_courts: List[Dict[str, Any]] = Field(..., description="Top courts citing this court")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Additional metadata")

class YearlyStats(BaseModel):
    """Model for yearly statistics."""
    year: int = Field(..., description="Year")
    opinion_count: int = Field(..., description="Number of opinions in this year")
    citation_count: int = Field(..., description="Number of citations in this year")
    cited_by_count: int = Field(..., description="Number of times opinions from this year are cited")
    avg_citations_per_opinion: float = Field(..., description="Average citations per opinion")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Additional metadata")

class TimelineStats(BaseModel):
    """Model for timeline statistics."""
    yearly_stats: List[YearlyStats] = Field(..., description="Statistics by year")
    total_years: int = Field(..., description="Total number of years")
    start_year: int = Field(..., description="First year in the timeline")
    end_year: int = Field(..., description="Last year in the timeline")

class TopCitedOpinion(BaseModel):
    """Model for a top cited opinion."""
    cluster_id: int = Field(..., description="Opinion cluster identifier")
    case_name: str = Field(..., description="Case name")
    court_id: str = Field(..., description="Court identifier")
    court_name: str = Field(..., description="Court name")
    date_filed: date = Field(..., description="Date the opinion was filed")
    citation_count: int = Field(..., description="Number of times this opinion has been cited")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Additional metadata")
    
    @classmethod
    def from_neo4j(cls, opinion: Neo4jOpinion, citation_count: int):
        """Create a TopCitedOpinion from a Neo4j Opinion model and citation count."""
        return cls(
            cluster_id=opinion.cluster_id,
            case_name=opinion.case_name or f"Opinion {opinion.cluster_id}",
            court_id=str(opinion.court_id),
            court_name=opinion.court_name or "Unknown Court",
            date_filed=opinion.date_filed,
            citation_count=citation_count,
            metadata={
                "docket_number": opinion.docket_number,
                "brief_summary": opinion.ai_summary
            }
        )
    
    model_config = ConfigDict(
        from_attributes=True  # Replaces deprecated orm_mode=True
    )

class TopCitedOpinions(BaseModel):
    """Model for top cited opinions."""
    opinions: List[TopCitedOpinion] = Field(..., description="List of top cited opinions")
    total_count: int = Field(..., description="Total number of opinions in the database")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Additional metadata")

================
File: src/api/routers/__init__.py
================
from . import opinions
from . import citations
from . import stats
from . import pipeline

__all__ = [
    'opinions',
    'citations',
    'stats',
    'pipeline'
]

================
File: src/api/routers/citations.py
================
from fastapi import APIRouter, Depends, HTTPException, Query
from typing import List, Optional
from neo4j import GraphDatabase

from ..database import get_neo4j
from ..models.citations import (
    CitationNetwork, 
    CitationDetail, 
    CitationStats
)
from ..services import citation_service

router = APIRouter(
    prefix="/api/citations",
    tags=["citations"],
    responses={404: {"description": "Not found"}},
)

@router.get("/network", response_model=CitationNetwork)
async def get_citation_network(
    cluster_id: Optional[int] = None,
    court_id: Optional[str] = None,
    depth: int = Query(1, ge=1, le=3),
    limit: int = Query(100, ge=1, le=500),
    neo4j_session = Depends(get_neo4j)
):
    """
    Get a citation network centered around a specific opinion or court.
    
    Args:
        cluster_id: Center the network on this opinion cluster ID
        court_id: Filter by court ID
        depth: Network depth (1-3)
        limit: Maximum number of nodes to return
        
    Returns:
        Citation network with nodes and edges
    """
    if not cluster_id and not court_id:
        raise HTTPException(
            status_code=400, 
            detail="Either cluster_id or court_id must be provided"
        )
    
    return citation_service.get_citation_network(
        neo4j_session,
        cluster_id=cluster_id,
        court_id=court_id,
        depth=depth,
        limit=limit
    )

@router.get("/{citing_id}/{cited_id}", response_model=CitationDetail)
async def get_citation_detail(
    citing_id: int,
    cited_id: int,
    neo4j_session = Depends(get_neo4j)
):
    """
    Get detailed information about a specific citation relationship.
    
    Args:
        citing_id: The citing opinion cluster ID
        cited_id: The cited opinion cluster ID
        
    Returns:
        Detailed citation information
    """
    citation = citation_service.get_citation_detail(
        neo4j_session, 
        citing_id, 
        cited_id
    )
    if citation is None:
        raise HTTPException(
            status_code=404, 
            detail=f"Citation relationship not found between {citing_id} and {cited_id}"
        )
    return citation

@router.get("/stats", response_model=CitationStats)
async def get_citation_stats(
    court_id: Optional[str] = None,
    year: Optional[int] = None,
    neo4j_session = Depends(get_neo4j)
):
    """
    Get citation statistics.
    
    Args:
        court_id: Filter by court ID
        year: Filter by year
        
    Returns:
        Citation statistics
    """
    return citation_service.get_citation_stats(
        neo4j_session,
        court_id=court_id,
        year=year
    )

@router.get("/influential", response_model=List[CitationDetail])
async def get_influential_citations(
    court_id: Optional[str] = None,
    limit: int = Query(20, ge=1, le=100),
    neo4j_session = Depends(get_neo4j)
):
    """
    Get the most influential citations based on citation count.
    
    Args:
        court_id: Filter by court ID
        limit: Maximum number of results to return
        
    Returns:
        List of influential citations
    """
    return citation_service.get_influential_citations(
        neo4j_session,
        court_id=court_id,
        limit=limit
    )

================
File: src/api/routers/opinions.py
================
from fastapi import APIRouter, Depends, HTTPException, Query
from sqlalchemy.orm import Session
from typing import List, Optional
from datetime import date

from ..database import get_db
from ..models.opinions import OpinionResponse, OpinionDetail
from ..services import opinion_service

router = APIRouter(
    prefix="/api/opinions",
    tags=["opinions"],
    responses={404: {"description": "Not found"}},
)

@router.get("/", response_model=List[OpinionResponse])
async def get_opinions(
    court_id: Optional[str] = None,
    start_date: Optional[date] = None,
    end_date: Optional[date] = None,
    limit: int = Query(20, ge=1, le=100),
    offset: int = Query(0, ge=0),
    db: Session = Depends(get_db)
):
    """
    Get a list of opinions with optional filtering.
    
    Args:
        court_id: Filter by court ID
        start_date: Filter by date range (start)
        end_date: Filter by date range (end)
        limit: Maximum number of results to return
        offset: Number of results to skip
        
    Returns:
        List of opinion summaries
    """
    return opinion_service.get_opinions(
        db, 
        court_id=court_id,
        start_date=start_date,
        end_date=end_date,
        limit=limit,
        offset=offset
    )

@router.get("/{cluster_id}", response_model=OpinionDetail)
async def get_opinion(cluster_id: int, db: Session = Depends(get_db)):
    """
    Get detailed information about a specific opinion.
    
    Args:
        cluster_id: The opinion cluster ID
        
    Returns:
        Detailed opinion information
    """
    opinion = opinion_service.get_opinion_by_cluster_id(db, cluster_id)
    if opinion is None:
        raise HTTPException(status_code=404, detail="Opinion not found")
    return opinion

@router.get("/{cluster_id}/text")
async def get_opinion_text(cluster_id: int, db: Session = Depends(get_db)):
    """
    Get the full text of an opinion.
    
    Args:
        cluster_id: The opinion cluster ID
        
    Returns:
        Full text of the opinion
    """
    text = opinion_service.get_opinion_text(db, cluster_id)
    if text is None:
        raise HTTPException(status_code=404, detail="Opinion text not found")
    return {"cluster_id": cluster_id, "text": text}

@router.get("/{cluster_id}/citations", response_model=List[OpinionResponse])
async def get_opinion_citations(
    cluster_id: int, 
    direction: str = Query("outgoing", regex="^(outgoing|incoming)$"),
    limit: int = Query(20, ge=1, le=100),
    db: Session = Depends(get_db)
):
    """
    Get opinions cited by or citing a specific opinion.
    
    Args:
        cluster_id: The opinion cluster ID
        direction: "outgoing" for opinions cited by this opinion, 
                  "incoming" for opinions citing this opinion
        limit: Maximum number of results to return
        
    Returns:
        List of related opinions
    """
    return opinion_service.get_opinion_citations(
        db, 
        cluster_id, 
        direction=direction,
        limit=limit
    )

================
File: src/api/routers/pipeline.py
================
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks, File, UploadFile, Query
from fastapi.security import APIKeyHeader
from sqlalchemy.orm import Session
from typing import List, Optional
import pandas as pd
import os
from datetime import datetime

from ..database import get_db, get_neo4j
from ..models.pipeline import (
    PipelineStatus, 
    PipelineJob, 
    PipelineResult,
    ExtractionConfig
)
from ..services import pipeline_service

router = APIRouter(
    prefix="/api/pipeline",
    tags=["pipeline"],
    responses={404: {"description": "Not found"}},
)

# Simple API key security for localhost-only endpoints
API_KEY = os.getenv("PIPELINE_API_KEY", "local_development_key")
api_key_header = APIKeyHeader(name="X-API-Key")

def verify_api_key(api_key: str = Depends(api_key_header)):
    if api_key != API_KEY:
        raise HTTPException(
            status_code=403, 
            detail="Invalid API key"
        )
    return True

@router.post("/extract", response_model=PipelineJob, dependencies=[Depends(verify_api_key)])
async def extract_opinions(
    config: ExtractionConfig,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """
    Extract opinions from PostgreSQL based on configuration.
    This is a long-running task that runs in the background.
    
    Args:
        config: Extraction configuration
        
    Returns:
        Job ID for tracking the extraction process
    """
    job_id = pipeline_service.create_job(db, "extract", config.dict())
    
    background_tasks.add_task(
        pipeline_service.run_extraction_job,
        db,
        job_id,
        config
    )
    
    return {"job_id": job_id, "status": "started"}

@router.post("/process-llm", response_model=PipelineJob, dependencies=[Depends(verify_api_key)])
async def process_opinions_with_llm(
    job_id: int,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """
    Process extracted opinions through the LLM for citation analysis.
    This is a long-running task that runs in the background.
    
    Args:
        job_id: ID of the extraction job to process
        
    Returns:
        Job ID for tracking the LLM processing
    """
    # Verify the extraction job exists and is complete
    extraction_job = pipeline_service.get_job(db, job_id)
    if not extraction_job:
        raise HTTPException(status_code=404, detail=f"Job {job_id} not found")
    
    if extraction_job.status != "completed":
        raise HTTPException(
            status_code=400, 
            detail=f"Extraction job {job_id} is not completed (status: {extraction_job.status})"
        )
    
    llm_job_id = pipeline_service.create_job(
        db, 
        "llm_process", 
        {"extraction_job_id": job_id}
    )
    
    background_tasks.add_task(
        pipeline_service.run_llm_job,
        db,
        llm_job_id,
        job_id
    )
    
    return {"job_id": llm_job_id, "status": "started"}

@router.post("/resolve-citations", response_model=PipelineJob, dependencies=[Depends(verify_api_key)])
async def resolve_citations(
    job_id: int,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """
    Resolve citations from LLM output to opinion cluster IDs.
    This is a long-running task that runs in the background.
    
    Args:
        job_id: ID of the LLM processing job
        
    Returns:
        Job ID for tracking the citation resolution
    """
    # Verify the LLM job exists and is complete
    llm_job = pipeline_service.get_job(db, job_id)
    if not llm_job:
        raise HTTPException(status_code=404, detail=f"Job {job_id} not found")
    
    if llm_job.status != "completed":
        raise HTTPException(
            status_code=400, 
            detail=f"LLM job {job_id} is not completed (status: {llm_job.status})"
        )
    
    resolution_job_id = pipeline_service.create_job(
        db, 
        "citation_resolution", 
        {"llm_job_id": job_id}
    )
    
    background_tasks.add_task(
        pipeline_service.run_resolution_job,
        db,
        resolution_job_id,
        job_id
    )
    
    return {"job_id": resolution_job_id, "status": "started"}

@router.post("/load-neo4j", response_model=PipelineJob, dependencies=[Depends(verify_api_key)])
async def load_neo4j(
    job_id: int,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    neo4j_session = Depends(get_neo4j)
):
    """
    Load resolved citations into Neo4j.
    This is a long-running task that runs in the background.
    
    Args:
        job_id: ID of the citation resolution job
        
    Returns:
        Job ID for tracking the Neo4j loading
    """
    # Verify the resolution job exists and is complete
    resolution_job = pipeline_service.get_job(db, job_id)
    if not resolution_job:
        raise HTTPException(status_code=404, detail=f"Job {job_id} not found")
    
    if resolution_job.status != "completed":
        raise HTTPException(
            status_code=400, 
            detail=f"Resolution job {job_id} is not completed (status: {resolution_job.status})"
        )
    
    neo4j_job_id = pipeline_service.create_job(
        db, 
        "neo4j_load", 
        {"resolution_job_id": job_id}
    )
    
    background_tasks.add_task(
        pipeline_service.run_neo4j_job,
        db,
        neo4j_session,
        neo4j_job_id,
        job_id
    )
    
    return {"job_id": neo4j_job_id, "status": "started"}

@router.post("/upload-csv", response_model=PipelineJob, dependencies=[Depends(verify_api_key)])
async def upload_csv(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    db: Session = Depends(get_db)
):
    """
    Upload a CSV file with opinions for processing.
    This is an alternative to the extract endpoint.
    
    Args:
        file: CSV file with opinions
        
    Returns:
        Job ID for tracking the upload process
    """
    # Create a temporary file to store the uploaded CSV
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"upload_{timestamp}_{file.filename}"
    file_path = os.path.join("/tmp", filename)
    
    # Save the uploaded file
    with open(file_path, "wb") as f:
        content = await file.read()
        f.write(content)
    
    # Create a job for the upload
    job_id = pipeline_service.create_job(
        db, 
        "csv_upload", 
        {"file_path": file_path, "original_filename": file.filename}
    )
    
    # Process the uploaded file in the background
    background_tasks.add_task(
        pipeline_service.process_uploaded_csv,
        db,
        job_id,
        file_path
    )
    
    return {"job_id": job_id, "status": "started"}

@router.get("/job/{job_id}", response_model=PipelineStatus, dependencies=[Depends(verify_api_key)])
async def get_job_status(job_id: int, db: Session = Depends(get_db)):
    """
    Get the status of a pipeline job.
    
    Args:
        job_id: Job ID
        
    Returns:
        Job status information
    """
    job = pipeline_service.get_job(db, job_id)
    if not job:
        raise HTTPException(status_code=404, detail=f"Job {job_id} not found")
    
    return job

@router.get("/jobs", response_model=List[PipelineStatus], dependencies=[Depends(verify_api_key)])
async def get_jobs(
    job_type: Optional[str] = None,
    status: Optional[str] = None,
    limit: int = Query(20, ge=1, le=100),
    offset: int = Query(0, ge=0),
    db: Session = Depends(get_db)
):
    """
    Get a list of pipeline jobs with optional filtering.
    
    Args:
        job_type: Filter by job type
        status: Filter by job status
        limit: Maximum number of jobs to return
        offset: Number of jobs to skip
        
    Returns:
        List of job status information
    """
    return pipeline_service.get_jobs(
        db,
        job_type=job_type,
        status=status,
        limit=limit,
        offset=offset
    )

@router.post("/run-full-pipeline", response_model=List[PipelineJob], dependencies=[Depends(verify_api_key)])
async def run_full_pipeline(
    config: ExtractionConfig,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    neo4j_session = Depends(get_neo4j)
):
    """
    Run the full pipeline from extraction to Neo4j loading.
    This is a convenience endpoint that chains all the steps.
    
    Args:
        config: Extraction configuration
        
    Returns:
        List of job IDs for each step in the pipeline
    """
    # Create jobs for each step
    extraction_job_id = pipeline_service.create_job(db, "extract", config.dict())
    llm_job_id = pipeline_service.create_job(db, "llm_process", {"extraction_job_id": extraction_job_id})
    resolution_job_id = pipeline_service.create_job(db, "citation_resolution", {"llm_job_id": llm_job_id})
    neo4j_job_id = pipeline_service.create_job(db, "neo4j_load", {"resolution_job_id": resolution_job_id})
    
    # Run the full pipeline in the background
    background_tasks.add_task(
        pipeline_service.run_full_pipeline,
        db,
        neo4j_session,
        extraction_job_id,
        llm_job_id,
        resolution_job_id,
        neo4j_job_id,
        config
    )
    
    return [
        {"job_id": extraction_job_id, "status": "queued"},
        {"job_id": llm_job_id, "status": "queued"},
        {"job_id": resolution_job_id, "status": "queued"},
        {"job_id": neo4j_job_id, "status": "queued"}
    ]

================
File: src/api/routers/stats.py
================
from fastapi import APIRouter, Depends, Query
from typing import List, Optional
from datetime import date
from neo4j import GraphDatabase

from ..database import get_neo4j
from ..models.stats import (
    NetworkStats, 
    CourtStats, 
    TimelineStats,
    TopCitedOpinions
)
from ..services import stats_service

router = APIRouter(
    prefix="/api/stats",
    tags=["stats"],
    responses={404: {"description": "Not found"}},
)

@router.get("/network", response_model=NetworkStats)
async def get_network_stats(neo4j_session = Depends(get_neo4j)):
    """
    Get overall statistics about the citation network.
    
    Returns:
        Overall network statistics
    """
    return stats_service.get_network_stats(neo4j_session)

@router.get("/courts", response_model=List[CourtStats])
async def get_court_stats(
    limit: int = Query(10, ge=1, le=50),
    neo4j_session = Depends(get_neo4j)
):
    """
    Get citation statistics by court.
    
    Args:
        limit: Maximum number of courts to return
        
    Returns:
        List of court statistics
    """
    return stats_service.get_court_stats(neo4j_session, limit=limit)

@router.get("/timeline", response_model=TimelineStats)
async def get_timeline_stats(
    court_id: Optional[str] = None,
    start_year: Optional[int] = None,
    end_year: Optional[int] = None,
    neo4j_session = Depends(get_neo4j)
):
    """
    Get citation statistics over time.
    
    Args:
        court_id: Filter by court ID
        start_year: Start year for timeline
        end_year: End year for timeline
        
    Returns:
        Timeline statistics
    """
    return stats_service.get_timeline_stats(
        neo4j_session,
        court_id=court_id,
        start_year=start_year,
        end_year=end_year
    )

@router.get("/top-cited", response_model=TopCitedOpinions)
async def get_top_cited_opinions(
    court_id: Optional[str] = None,
    start_date: Optional[date] = None,
    end_date: Optional[date] = None,
    limit: int = Query(20, ge=1, le=100),
    neo4j_session = Depends(get_neo4j)
):
    """
    Get the most cited opinions.
    
    Args:
        court_id: Filter by court ID
        start_date: Filter by date range (start)
        end_date: Filter by date range (end)
        limit: Maximum number of opinions to return
        
    Returns:
        List of top cited opinions
    """
    return stats_service.get_top_cited_opinions(
        neo4j_session,
        court_id=court_id,
        start_date=start_date,
        end_date=end_date,
        limit=limit
    )

@router.get("/citation-distribution")
async def get_citation_distribution(
    bins: int = Query(10, ge=5, le=50),
    neo4j_session = Depends(get_neo4j)
):
    """
    Get the distribution of citation counts.
    
    Args:
        bins: Number of bins for the distribution
        
    Returns:
        Citation count distribution
    """
    return stats_service.get_citation_distribution(
        neo4j_session,
        bins=bins
    )

================
File: src/api/services/__init__.py
================
from . import db_utils
from . import opinion_service
from . import citation_service
from . import stats_service
from . import pipeline_service

__all__ = [
    'db_utils',
    'opinion_service',
    'citation_service',
    'stats_service',
    'pipeline_service'
]

================
File: src/api/services/citation_service.py
================
from typing import List, Dict, Optional, Any
import logging
from datetime import datetime

from src.neo4j.models import Opinion as Neo4jOpinion, CitesRel
from src.llm_extraction.models import Citation, CitationAnalysis
from .db_utils import get_opinion_by_id, get_filtered_opinions

logger = logging.getLogger(__name__)

def opinion_to_node_dict(opinion: Neo4jOpinion, node_type: str = "opinion") -> dict:
    """Helper to convert a Neo4j Opinion to a node dictionary."""
    return {
        "id": opinion.cluster_id,
        "label": opinion.case_name or f"Opinion {opinion.cluster_id}",
        "type": node_type,
        "court_id": opinion.court_id,
        "date_filed": opinion.date_filed.isoformat() if opinion.date_filed else None,
        "citation_count": len(opinion.cited_by.all()) if hasattr(opinion, 'cited_by') else 0
    }

def get_citation_network(
    neo4j_session,  # Kept for API compatibility but not used
    cluster_id: Optional[int] = None,
    court_id: Optional[str] = None,
    depth: int = 1,
    limit: int = 100
) -> Dict[str, Any]:
    """Get a citation network centered around a specific opinion or court using neomodel."""
    depth = min(max(depth, 1), 3)  # Clamp depth between 1-3
    nodes = []
    edges = []
    
    try:
        if cluster_id:
            # Get center opinion using simplified db_utils
            center_opinion = get_opinion_by_id(cluster_id)
            if not center_opinion:
                return {"nodes": [], "edges": [], "metadata": {}}
                
            nodes.append(opinion_to_node_dict(center_opinion, "center"))
            
            # Get outgoing citations using relationship traversal
            cited_opinions = set()
            current_level = {center_opinion}
            
            for _ in range(depth):
                next_level = set()
                for opinion in current_level:
                    # Get direct citations limited by the limit parameter
                    new_cited = set(opinion.cites.all()[:limit // len(current_level)])
                    
                    for cited in new_cited:
                        if cited.cluster_id not in {n["id"] for n in nodes}:
                            nodes.append(opinion_to_node_dict(cited, "cited"))
                        
                        # Add edge using relationship properties
                        rel = opinion.cites.relationship(cited)
                        if rel:
                            edges.append({
                                "source": opinion.cluster_id,
                                "target": cited.cluster_id,
                                "treatment": rel.treatment,
                                "relevance": rel.relevance
                            })
                    
                    next_level.update(new_cited)
                cited_opinions.update(next_level)
                current_level = next_level
                
            # Get incoming citations similarly
            citing_opinions = set()
            current_level = {center_opinion}
            
            for _ in range(depth):
                next_level = set()
                for opinion in current_level:
                    new_citing = set(opinion.cited_by.all()[:limit // len(current_level)])
                    
                    for citing in new_citing:
                        if citing.cluster_id not in {n["id"] for n in nodes}:
                            nodes.append(opinion_to_node_dict(citing, "citing"))
                        
                        rel = citing.cites.relationship(opinion)
                        if rel:
                            edges.append({
                                "source": citing.cluster_id,
                                "target": opinion.cluster_id,
                                "treatment": rel.treatment,
                                "relevance": rel.relevance
                            })
                    
                    next_level.update(new_citing)
                citing_opinions.update(next_level)
                current_level = next_level

        elif court_id:
            # Get all opinions from this court using simplified db_utils
            court_opinions = get_filtered_opinions(
                court_id=court_id,
                limit=limit
            )
            
            for opinion in court_opinions:
                if opinion.cluster_id not in {n["id"] for n in nodes}:
                    nodes.append(opinion_to_node_dict(opinion, "court"))
                
                # Get direct citations and citing opinions
                for cited in opinion.cites.all()[:limit // len(court_opinions)]:
                    if cited.cluster_id not in {n["id"] for n in nodes}:
                        nodes.append(opinion_to_node_dict(cited))
                    
                    rel = opinion.cites.relationship(cited)
                    if rel:
                        edges.append({
                            "source": opinion.cluster_id,
                            "target": cited.cluster_id,
                            "treatment": rel.treatment,
                            "relevance": rel.relevance
                        })
                
                for citing in opinion.cited_by.all()[:limit // len(court_opinions)]:
                    if citing.cluster_id not in {n["id"] for n in nodes}:
                        nodes.append(opinion_to_node_dict(citing))
                    
                    rel = citing.cites.relationship(opinion)
                    if rel:
                        edges.append({
                            "source": citing.cluster_id,
                            "target": opinion.cluster_id,
                            "treatment": rel.treatment,
                            "relevance": rel.relevance
                        })
        
        return {
            "nodes": nodes,
            "edges": edges,
            "metadata": {
                "center_id": cluster_id,
                "court_id": court_id,
                "depth": depth,
                "node_count": len(nodes),
                "edge_count": len(edges)
            }
        }
    except Neo4jOpinion.DoesNotExist:
        logger.warning(f"Opinion not found: cluster_id={cluster_id}")
        return {"nodes": [], "edges": [], "metadata": {}}
    except Exception as e:
        logger.error(f"Error getting citation network: {str(e)}")
        return {"nodes": [], "edges": [], "metadata": {}}

def get_citation_detail(
    neo4j_session,  # Kept for API compatibility but not used
    citing_id: int,
    cited_id: int
) -> Optional[Dict[str, Any]]:
    """Get citation details using neomodel relationship traversal."""
    try:
        # Use db_utils to get opinions by ID
        citing_opinion = get_opinion_by_id(citing_id)
        cited_opinion = get_opinion_by_id(cited_id)
        
        if not citing_opinion or not cited_opinion:
            return None
        
        rel = citing_opinion.cites.relationship(cited_opinion)
        if not rel:
            return None
        
        return {
            "citing_opinion": opinion_to_node_dict(citing_opinion),
            "cited_opinion": opinion_to_node_dict(cited_opinion),
            "citation": {
                "text": rel.citation_text,
                "page_number": rel.page_number,
                "treatment": rel.treatment,
                "relevance": rel.relevance,
                "reasoning": rel.reasoning,
                "section": rel.opinion_section,
                "source": rel.source
            }
        }
    except Exception as e:
        logger.error(f"Error getting citation detail: {str(e)}")
        return None

def get_citation_stats(
    neo4j_session,  # Kept for API compatibility but not used
    court_id: Optional[str] = None,
    year: Optional[int] = None
) -> Dict[str, Any]:
    """Get citation statistics using neomodel filters and relationship traversal."""
    try:
        # Build filter conditions
        filters = {}
        if court_id:
            filters["court_id"] = court_id
        if year:
            filters["date_filed__year"] = year
            
        # Use db_utils to get filtered opinions
        opinions = get_filtered_opinions(
            court_id=court_id,
            limit=None,  # No limit for stats calculation
            **filters
        )
        
        # Calculate stats using relationship traversal
        total_opinions = len(opinions)
        total_citations = 0
        treatment_counts = {}
        relevance_distribution = {}
        section_distribution = {}
        
        for opinion in opinions:
            citations = opinion.cites.all()
            total_citations += len(citations)
            
            for cited in citations:
                rel = opinion.cites.relationship(cited)
                if rel.treatment:
                    treatment_counts[rel.treatment] = treatment_counts.get(rel.treatment, 0) + 1
                if rel.relevance:
                    relevance_distribution[rel.relevance] = relevance_distribution.get(rel.relevance, 0) + 1
                if rel.opinion_section:
                    section_distribution[rel.opinion_section] = section_distribution.get(rel.opinion_section, 0) + 1
        
        return {
            "total_citations": total_citations,
            "total_opinions": total_opinions,
            "avg_citations_per_opinion": total_citations / total_opinions if total_opinions > 0 else 0,
            "treatment_counts": treatment_counts,
            "relevance_distribution": relevance_distribution,
            "section_distribution": section_distribution,
            "metadata": {"court_id": court_id, "year": year}
        }
    except Exception as e:
        logger.error(f"Error getting citation stats: {str(e)}")
        return {
            "total_citations": 0,
            "total_opinions": 0,
            "avg_citations_per_opinion": 0,
            "treatment_counts": {},
            "relevance_distribution": {},
            "section_distribution": {},
            "metadata": {}
        }

def get_influential_citations(
    neo4j_session,  # Kept for API compatibility but not used
    court_id: Optional[str] = None,
    limit: int = 20
) -> List[Dict[str, Any]]:
    """Get influential citations using neomodel filters and relationship traversal."""
    try:
        # Use db_utils to get filtered opinions
        opinions = get_filtered_opinions(court_id=court_id, limit=None)
        
        # Get citation counts using relationship traversal
        citation_counts = {
            opinion.cluster_id: (opinion, len(opinion.cited_by.all()))
            for opinion in opinions
        }
        
        # Sort and get top cited
        top_cited = sorted(citation_counts.items(), key=lambda x: x[1][1], reverse=True)[:limit]
        
        influential = []
        for _, (cited_opinion, citation_count) in top_cited:
            # Use neomodel's order_by for citing opinions
            citing_opinions = cited_opinion.cited_by.order_by("-date_filed")
            
            if citing_opinions:
                recent_citing = citing_opinions[0]
                rel = recent_citing.cites.relationship(cited_opinion)
                
                if rel:
                    influential.append({
                        "cited_opinion": {
                            "cluster_id": cited_opinion.cluster_id,
                            "case_name": cited_opinion.case_name,
                            "citation_count": citation_count
                        },
                        "recent_citation": {
                            "citing_opinion": opinion_to_node_dict(recent_citing),
                            "treatment": rel.treatment,
                            "relevance": rel.relevance
                        }
                    })
        
        return influential
    
    except Exception as e:
        logger.error(f"Error getting influential citations: {str(e)}")
        return []

================
File: src/api/services/db_utils.py
================
"""
Simple database utilities for service layer.

This module provides common database access patterns and error handling
to reduce duplication across service modules.
"""

import logging
from typing import List, Dict, Optional, Any
from datetime import date

from src.neo4j.models import Opinion as Neo4jOpinion

logger = logging.getLogger(__name__)

# --- Neo4j Utilities ---

def get_opinion_by_id(cluster_id: int) -> Optional[Neo4jOpinion]:
    """
    Get a Neo4j Opinion by cluster_id.
    
    Args:
        cluster_id: Opinion cluster ID
        
    Returns:
        Opinion object or None if not found
    """
    try:
        return Neo4jOpinion.nodes.get(cluster_id=cluster_id)
    except Neo4jOpinion.DoesNotExist:
        logger.warning(f"Opinion with cluster_id {cluster_id} not found")
        return None
    except Exception as e:
        logger.error(f"Error getting opinion {cluster_id}: {str(e)}")
        return None

def get_filtered_opinions(
    court_id: Optional[str] = None,
    start_date: Optional[date] = None,
    end_date: Optional[date] = None,
    order_by: str = "-date_filed",
    limit: int = 20,
    offset: int = 0,
    **additional_filters
) -> List[Neo4jOpinion]:
    """
    Get filtered opinions with pagination.
    
    Args:
        court_id: Filter by court ID
        start_date: Filter by start date
        end_date: Filter by end date
        order_by: Field to order by
        limit: Max number of results
        offset: Number of results to skip
        additional_filters: Any additional filters
        
    Returns:
        List of opinions
    """
    try:
        # Build query filters
        filters = {}
        
        if court_id:
            filters["court_id"] = court_id
        
        if start_date:
            filters["date_filed__gte"] = start_date
        
        if end_date:
            filters["date_filed__lte"] = end_date
            
        # Add any additional filters
        filters.update(additional_filters)
        
        # Get filtered and ordered opinions
        if filters:
            opinions = Neo4jOpinion.nodes.filter(**filters)
        else:
            opinions = Neo4jOpinion.nodes
            
        if order_by:
            opinions = opinions.order_by(order_by)
            
        # Apply pagination
        return opinions[offset:offset + limit]
    
    except Exception as e:
        logger.error(f"Error getting filtered opinions: {str(e)}")
        return []

# --- PostgreSQL Utilities ---

def get_postgres_entity_by_id(db_session, model_class, id_field: str, id_value: Any):
    """
    Get a PostgreSQL entity by ID.
    
    Args:
        db_session: SQLAlchemy session
        model_class: Model class
        id_field: ID field name
        id_value: ID value
        
    Returns:
        Entity or None if not found
    """
    try:
        return db_session.query(model_class).filter(
            getattr(model_class, id_field) == id_value
        ).first()
    except Exception as e:
        logger.error(f"Error getting {model_class.__name__} with {id_field}={id_value}: {str(e)}")
        return None

================
File: src/api/services/opinion_service.py
================
from typing import List, Optional
from datetime import date
import logging

from ..models.opinions import OpinionResponse, OpinionDetail, OpinionBase
from src.neo4j.models import Opinion as Neo4jOpinion
from .db_utils import get_opinion_by_id, get_filtered_opinions, get_postgres_entity_by_id

logger = logging.getLogger(__name__)

def get_opinions(
    db,
    court_id: Optional[str] = None,
    start_date: Optional[date] = None,
    end_date: Optional[date] = None,
    limit: int = 20,
    offset: int = 0
) -> List[OpinionResponse]:
    """
    Get a list of opinions with optional filtering.
    
    Args:
        db: Database session (unused, kept for API compatibility)
        court_id: Filter by court ID
        start_date: Filter by date range (start)
        end_date: Filter by date range (end)
        limit: Maximum number of results to return
        offset: Number of results to skip
        
    Returns:
        List of opinion summaries
    """
    # Use the db_utils function for filtered queries
    opinions = get_filtered_opinions(
        court_id=court_id,
        start_date=start_date,
        end_date=end_date,
        limit=limit,
        offset=offset
    )
    
    # Convert to response models
    return [OpinionResponse.from_neo4j(opinion) for opinion in opinions]

def get_opinion_by_cluster_id(db, cluster_id: int) -> Optional[OpinionDetail]:
    """
    Get detailed information about a specific opinion.
    
    Args:
        db: Database session (unused, kept for API compatibility)
        cluster_id: The opinion cluster ID
        
    Returns:
        Detailed opinion information or None if not found
    """
    # Use the db_utils function to get entity by ID
    opinion = get_opinion_by_id(cluster_id)
    
    # Convert to response model if found
    if opinion:
        return OpinionDetail.from_neo4j(opinion)
    return None

def get_opinion_text(db, cluster_id: int) -> Optional[str]:
    """
    Get the full text of an opinion from PostgreSQL.
    
    Args:
        db: PostgreSQL database session
        cluster_id: The opinion cluster ID
        
    Returns:
        Full text of the opinion or None if not found
    """
    # Import models here to avoid circular imports
    from src.postgres.models import OpinionClusterExtraction, OpinionText
    
    # Get the opinion cluster using the db_utils function
    opinion_cluster = get_postgres_entity_by_id(
        db, 
        OpinionClusterExtraction, 
        "cluster_id", 
        cluster_id
    )
    
    if not opinion_cluster or not opinion_cluster.opinion_text:
        logger.warning(f"Opinion text for cluster_id {cluster_id} not found in PostgreSQL")
        return None
        
    return opinion_cluster.opinion_text.text

def get_opinion_citations(
    db,
    cluster_id: int,
    direction: str = "outgoing",
    limit: int = 20
) -> List[OpinionResponse]:
    """
    Get opinions cited by or citing a specific opinion.
    
    Args:
        db: Database session (unused, kept for API compatibility)
        cluster_id: The opinion cluster ID
        direction: "outgoing" for opinions cited by this opinion, 
                  "incoming" for opinions citing this opinion
        limit: Maximum number of results to return
        
    Returns:
        List of related opinions
    """
    # Use db_utils to get the opinion by ID
    opinion = get_opinion_by_id(cluster_id)
    
    if not opinion:
        return []
    
    try:
        # Get related opinions based on direction
        if direction == "outgoing":
            # Get opinions cited by this opinion
            related_opinions = opinion.cites.all()[:limit]
        else:
            # Get opinions citing this opinion
            related_opinions = opinion.cited_by.all()[:limit]
        
        # Convert to response models
        return [OpinionResponse.from_neo4j(related) for related in related_opinions]
    
    except Exception as e:
        logger.error(f"Error getting citations for opinion {cluster_id}: {str(e)}")
        return []

================
File: src/api/services/pipeline_service.py
================
import os
import json
import logging
import pandas as pd
from typing import List, Dict, Optional, Any
from datetime import datetime
from sqlalchemy.orm import Session
from sqlalchemy import desc
import time

from ..models.pipeline import JobStatus, JobType, ExtractionConfig
from src.llm_extraction.rate_limited_gemini import GeminiClient
from src.llm_extraction.models import CombinedResolvedCitationAnalysis
from src.neo4j.neomodel_loader import NeomodelLoader

logger = logging.getLogger(__name__)

# Define a simple job model for tracking pipeline jobs
# In a production environment, this would be a proper SQLAlchemy model
class PipelineJob:
    """Simple in-memory job model for tracking pipeline jobs."""
    
    def __init__(
        self, 
        job_id: int, 
        job_type: str, 
        config: Dict[str, Any],
        status: str = JobStatus.QUEUED
    ):
        self.job_id = job_id
        self.job_type = job_type
        self.config = config
        self.status = status
        self.created_at = datetime.now()
        self.started_at = None
        self.completed_at = None
        self.progress = 0.0
        self.message = None
        self.error = None
        self.result_path = None

# In-memory job storage (for demonstration)
# In a production environment, this would be stored in a database using SQLAlchemy models
_jobs = {}
_next_job_id = 1

def create_job(db: Session, job_type: str, config: Dict[str, Any]) -> int:
    """
    Create a new pipeline job.
    
    Args:
        db: Database session
        job_type: Type of job
        config: Job configuration
        
    Returns:
        Job ID
    """
    global _next_job_id
    
    # In a production environment, this would create a database record
    job_id = _next_job_id
    _next_job_id += 1
    
    job = PipelineJob(job_id, job_type, config)
    _jobs[job_id] = job
    
    logger.info(f"Created {job_type} job with ID {job_id}")
    return job_id

def get_job(db: Session, job_id: int) -> Optional[Dict[str, Any]]:
    """
    Get job status.
    
    Args:
        db: Database session
        job_id: Job ID
        
    Returns:
        Job status information
    """
    # In a production environment, this would query the database
    if job_id not in _jobs:
        return None
    
    job = _jobs[job_id]
    return {
        "job_id": job.job_id,
        "job_type": job.job_type,
        "status": job.status,
        "created_at": job.created_at,
        "started_at": job.started_at,
        "completed_at": job.completed_at,
        "config": job.config,
        "progress": job.progress,
        "message": job.message,
        "error": job.error,
        "result_path": job.result_path
    }

def get_jobs(
    db: Session,
    job_type: Optional[str] = None,
    status: Optional[str] = None,
    limit: int = 20,
    offset: int = 0
) -> List[Dict[str, Any]]:
    """
    Get a list of jobs with optional filtering.
    
    Args:
        db: Database session
        job_type: Filter by job type
        status: Filter by job status
        limit: Maximum number of jobs to return
        offset: Number of jobs to skip
        
    Returns:
        List of job status information
    """
    # In a production environment, this would query the database
    jobs = list(_jobs.values())
    
    # Apply filters
    if job_type:
        jobs = [job for job in jobs if job.job_type == job_type]
    
    if status:
        jobs = [job for job in jobs if job.status == status]
    
    # Sort by creation time (newest first)
    jobs.sort(key=lambda job: job.created_at, reverse=True)
    
    # Apply pagination
    jobs = jobs[offset:offset + limit]
    
    # Convert to dictionaries
    return [
        {
            "job_id": job.job_id,
            "job_type": job.job_type,
            "status": job.status,
            "created_at": job.created_at,
            "started_at": job.started_at,
            "completed_at": job.completed_at,
            "config": job.config,
            "progress": job.progress,
            "message": job.message,
            "error": job.error,
            "result_path": job.result_path
        }
        for job in jobs
    ]

def update_job_status(
    db: Session,
    job_id: int,
    status: str,
    progress: Optional[float] = None,
    message: Optional[str] = None,
    error: Optional[str] = None,
    result_path: Optional[str] = None
) -> None:
    """
    Update job status.
    
    Args:
        db: Database session
        job_id: Job ID
        status: New job status
        progress: Job progress (0-100)
        message: Status message
        error: Error message
        result_path: Path to job result file
    """
    # In a production environment, this would update a database record
    if job_id not in _jobs:
        logger.warning(f"Attempted to update non-existent job {job_id}")
        return
    
    job = _jobs[job_id]
    job.status = status
    
    if progress is not None:
        job.progress = progress
    
    if message is not None:
        job.message = message
    
    if error is not None:
        job.error = error
    
    if result_path is not None:
        job.result_path = result_path
    
    # Update timestamps
    if status == JobStatus.STARTED and job.started_at is None:
        job.started_at = datetime.now()
    
    if status in [JobStatus.COMPLETED, JobStatus.FAILED] and job.completed_at is None:
        job.completed_at = datetime.now()
    
    logger.info(f"Updated job {job_id} status to {status}")

def run_extraction_job(db: Session, job_id: int, config: ExtractionConfig) -> None:
    """
    Run an opinion extraction job.
    
    Args:
        db: Database session
        job_id: Job ID
        config: Extraction configuration
    """
    try:
        # Update job status
        update_job_status(
            db, 
            job_id, 
            JobStatus.STARTED, 
            progress=0.0,
            message="Starting opinion extraction"
        )
        
        # Build SQL query based on configuration
        filters = []
        params = {}
        
        if config.court_id:
            filters.append("sd.court_id = :court_id")
            params["court_id"] = config.court_id
        
        if config.start_date:
            filters.append("soc.date_filed >= :start_date")
            params["start_date"] = config.start_date
        
        if config.end_date:
            filters.append("soc.date_filed <= :end_date")
            params["end_date"] = config.end_date
        
        filter_clause = " AND ".join(filters)
        if filter_clause:
            filter_clause = f"WHERE {filter_clause} AND soc.precedential_status = 'Published'"
        else:
            filter_clause = "WHERE soc.precedential_status = 'Published'"
        
        # Build the query
        query = f"""
        SELECT  
            so.cluster_id as cluster_id, 
            so.type as so_type, 
            so.id as so_id, 
            so.page_count as so_page_count, 
            so.plain_text as so_plain_text, 
            soc.case_name as cluster_case_name,
            soc.date_filed as soc_date_filed,
            sd.court_id as court_id,
            sc.full_name as court_name
        FROM search_opinion so 
        LEFT JOIN search_opinioncluster soc ON so.cluster_id = soc.id
        LEFT JOIN search_docket sd ON soc.docket_id = sd.id
        LEFT JOIN search_court sc ON sd.court_id = sc.id
        {filter_clause}
        ORDER BY soc.date_filed DESC
        """
        
        if config.limit:
            query += f" LIMIT {config.limit}"
        
        if config.offset:
            query += f" OFFSET {config.offset}"
        
        # Execute query
        update_job_status(
            db, 
            job_id, 
            JobStatus.PROCESSING, 
            progress=10.0,
            message="Executing database query"
        )
        
        # In a production environment, this would use the actual database connection
        # For demonstration, we'll simulate the query result
        df = pd.read_sql(query, db.bind, params=params)
        
        # Simulate query result
        # df = pd.DataFrame({
        #     "cluster_id": [1001, 1002, 1003],
        #     "so_type": ["010combined", "010combined", "010combined"],
        #     "so_id": [2001, 2002, 2003],
        #     "so_page_count": [10, 15, 20],
        #     "so_plain_text": [
        #         "This is the text of opinion 1001...",
        #         "This is the text of opinion 1002...",
        #         "This is the text of opinion 1003..."
        #     ],
        #     "cluster_case_name": [
        #         "Smith v. Jones",
        #         "Brown v. Board of Education",
        #         "Roe v. Wade"
        #     ],
        #     "soc_date_filed": [
        #         "2020-01-01",
        #         "2020-02-01",
        #         "2020-03-01"
        #     ],
        #     "court_id": ["scotus", "scotus", "scotus"],
        #     "court_name": [
        #         "Supreme Court of the United States",
        #         "Supreme Court of the United States",
        #         "Supreme Court of the United States"
        #     ]
        # })
        
        # Save to CSV
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"extracted_opinions_{timestamp}.csv"
        output_path = os.path.join("/tmp", output_file)
        
        df.to_csv(output_path, index=False)
        
        # Update job status
        update_job_status(
            db, 
            job_id, 
            JobStatus.COMPLETED, 
            progress=100.0,
            message=f"Extracted {len(df)} opinions",
            result_path=output_path
        )
        
        logger.info(f"Completed extraction job {job_id}, saved to {output_path}")
        
    except Exception as e:
        logger.error(f"Error in extraction job {job_id}: {str(e)}")
        update_job_status(
            db, 
            job_id, 
            JobStatus.FAILED, 
            error=str(e)
        )

def run_llm_job(db: Session, job_id: int, extraction_job_id: int) -> None:
    """
    Run an LLM processing job.
    
    Args:
        db: Database session
        job_id: Job ID
        extraction_job_id: ID of the extraction job to process
    """
    try:
        # Update job status
        update_job_status(
            db, 
            job_id, 
            JobStatus.STARTED, 
            progress=0.0,
            message="Starting LLM processing"
        )
        
        # Get extraction job
        extraction_job = get_job(db, extraction_job_id)
        if not extraction_job:
            raise ValueError(f"Extraction job {extraction_job_id} not found")
        
        if extraction_job["status"] != JobStatus.COMPLETED:
            raise ValueError(f"Extraction job {extraction_job_id} is not completed")
        
        # Load extracted opinions
        update_job_status(
            db, 
            job_id, 
            JobStatus.PROCESSING, 
            progress=10.0,
            message="Loading extracted opinions"
        )
        
        # In a production environment, this would load the actual CSV file
        # For demonstration, we'll simulate the data
        df = pd.read_csv(extraction_job["result_path"])
        
        # # Simulate data
        # df = pd.DataFrame({
        #     "cluster_id": [1001, 1002, 1003],
        #     "so_plain_text": [
        #         "This is the text of opinion 1001...",
        #         "This is the text of opinion 1002...",
        #         "This is the text of opinion 1003..."
        #     ]
        # })
        
        # Initialize Gemini client
        update_job_status(
            db, 
            job_id, 
            JobStatus.PROCESSING, 
            progress=20.0,
            message="Initializing LLM client"
        )
        
        # In a production environment, this would use the actual Gemini client
        # For demonstration, we'll simulate the processing
        gemini_client = GeminiClient(
            api_key=os.getenv("GEMINI_API_KEY"),
            rpm_limit=15,
            max_concurrent=10
        )
        
        # Process opinions
        update_job_status(
            db, 
            job_id, 
            JobStatus.PROCESSING, 
            progress=30.0,
            message=f"Processing {len(df)} opinions with LLM"
        )
        
        # In a production environment, this would process the opinions with Gemini
        # For demonstration, we'll simulate the results
        results = gemini_client.process_dataframe(
            df,
            text_column="text",
            max_workers=10
        )
        
        # Simulate results
        # results = {
        #     1001: [{"parsed": {"date": "2020-01-01", "brief_summary": "Summary 1", "majority_opinion_citations": []}}],
        #     1002: [{"parsed": {"date": "2020-02-01", "brief_summary": "Summary 2", "majority_opinion_citations": []}}],
        #     1003: [{"parsed": {"date": "2020-03-01", "brief_summary": "Summary 3", "majority_opinion_citations": []}}]
        # }
        
        # Save results
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"llm_results_{timestamp}.json"
        output_path = os.path.join("/tmp", output_file)
        
        with open(output_path, "w") as f:
            json.dump(results, f)
        
        # Update job status
        update_job_status(
            db, 
            job_id, 
            JobStatus.COMPLETED, 
            progress=100.0,
            message=f"Processed {len(results)} opinions with LLM",
            result_path=output_path
        )
        
        logger.info(f"Completed LLM job {job_id}, saved to {output_path}")
        
    except Exception as e:
        logger.error(f"Error in LLM job {job_id}: {str(e)}")
        update_job_status(
            db, 
            job_id, 
            JobStatus.FAILED, 
            error=str(e)
        )

def run_resolution_job(db: Session, job_id: int, llm_job_id: int) -> None:
    """
    Run a citation resolution job.
    
    Args:
        db: Database session
        job_id: Job ID
        llm_job_id: ID of the LLM job to process
    """
    try:
        # Update job status
        update_job_status(
            db, 
            job_id, 
            JobStatus.STARTED, 
            progress=0.0,
            message="Starting citation resolution"
        )
        
        # Get LLM job
        llm_job = get_job(db, llm_job_id)
        if not llm_job:
            raise ValueError(f"LLM job {llm_job_id} not found")
        
        if llm_job["status"] != JobStatus.COMPLETED:
            raise ValueError(f"LLM job {llm_job_id} is not completed")
        
        # Load LLM results
        update_job_status(
            db, 
            job_id, 
            JobStatus.PROCESSING, 
            progress=10.0,
            message="Loading LLM results"
        )
        
        # In a production environment, this would load the actual JSON file
        # For demonstration, we'll simulate the data
        # with open(llm_job["result_path"], "r") as f:
        #     llm_results = json.load(f)
        
        # Simulate results
        llm_results = {
            "1001": [{"parsed": {"date": "2020-01-01", "brief_summary": "Summary 1", "majority_opinion_citations": []}}],
            "1002": [{"parsed": {"date": "2020-02-01", "brief_summary": "Summary 2", "majority_opinion_citations": []}}],
            "1003": [{"parsed": {"date": "2020-03-01", "brief_summary": "Summary 3", "majority_opinion_citations": []}}]
        }
        
        # Resolve citations
        update_job_status(
            db, 
            job_id, 
            JobStatus.PROCESSING, 
            progress=30.0,
            message="Resolving citations"
        )
        
        # In a production environment, this would resolve the citations
        # For demonstration, we'll simulate the results
        resolved_citations = []
        for cluster_id, results in llm_results.items():
            try:
                # Create CombinedResolvedCitationAnalysis
                citation_analysis = CombinedResolvedCitationAnalysis.from_citations_json(
                    results, int(cluster_id)
                )
                resolved_citations.append(citation_analysis)
            except Exception as e:
                logger.warning(f"Error resolving citations for cluster {cluster_id}: {str(e)}")
        
        # Save results
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"resolved_citations_{timestamp}.json"
        output_path = os.path.join("/tmp", output_file)
        
        with open(output_path, "w") as f:
            json.dump(
                [citation.model_dump() for citation in resolved_citations],
                f
            )
        
        # Update job status
        update_job_status(
            db, 
            job_id, 
            JobStatus.COMPLETED, 
            progress=100.0,
            message=f"Resolved citations for {len(resolved_citations)} opinions",
            result_path=output_path
        )
        
        logger.info(f"Completed resolution job {job_id}, saved to {output_path}")
        
    except Exception as e:
        logger.error(f"Error in resolution job {job_id}: {str(e)}")
        update_job_status(
            db, 
            job_id, 
            JobStatus.FAILED, 
            error=str(e)
        )

def run_neo4j_job(db: Session, neo4j_session, job_id: int, resolution_job_id: int) -> None:
    """
    Run a Neo4j loading job.
    
    Args:
        db: Database session
        neo4j_session: Neo4j session
        job_id: Job ID
        resolution_job_id: ID of the resolution job to process
    """
    try:
        # Update job status
        update_job_status(
            db, 
            job_id, 
            JobStatus.STARTED, 
            progress=0.0,
            message="Starting Neo4j loading"
        )
        
        # Get resolution job
        resolution_job = get_job(db, resolution_job_id)
        if not resolution_job:
            raise ValueError(f"Resolution job {resolution_job_id} not found")
        
        if resolution_job["status"] != JobStatus.COMPLETED:
            raise ValueError(f"Resolution job {resolution_job_id} is not completed")
        
        # Load resolved citations
        update_job_status(
            db, 
            job_id, 
            JobStatus.PROCESSING, 
            progress=10.0,
            message="Loading resolved citations"
        )
        
        # In a production environment, this would load the actual JSON file
        # For demonstration, we'll simulate the data
        # with open(resolution_job["result_path"], "r") as f:
        #     resolved_citations_data = json.load(f)
        #     resolved_citations = [
        #         CombinedResolvedCitationAnalysis.model_validate(data)
        #         for data in resolved_citations_data
        #     ]
        
        # Simulate data
        resolved_citations = [
            CombinedResolvedCitationAnalysis(
                date="2020-01-01",
                cluster_id=1001,
                brief_summary="Summary 1",
                majority_opinion_citations=[],
                concurrent_opinion_citations=[],
                dissenting_citations=[]
            ),
            CombinedResolvedCitationAnalysis(
                date="2020-02-01",
                cluster_id=1002,
                brief_summary="Summary 2",
                majority_opinion_citations=[],
                concurrent_opinion_citations=[],
                dissenting_citations=[]
            ),
            CombinedResolvedCitationAnalysis(
                date="2020-03-01",
                cluster_id=1003,
                brief_summary="Summary 3",
                majority_opinion_citations=[],
                concurrent_opinion_citations=[],
                dissenting_citations=[]
            )
        ]
        
        # Initialize Neo4j loader
        update_job_status(
            db, 
            job_id, 
            JobStatus.PROCESSING, 
            progress=30.0,
            message="Initializing Neo4j loader"
        )
        
        # In a production environment, this would use the actual Neo4j loader
        # For demonstration, we'll simulate the loading
        # loader = NeomodelLoader(
        #     uri=os.getenv("NEO4J_URI", "localhost:7687"),
        #     username=os.getenv("NEO4J_USER", "neo4j"),
        #     password=os.getenv("NEO4J_PASSWORD", "courtlistener"),
        #     database=os.getenv("NEO4J_DATABASE", "courtlistener")
        # )
        
        # Load citations
        update_job_status(
            db, 
            job_id, 
            JobStatus.PROCESSING, 
            progress=50.0,
            message="Loading citations into Neo4j"
        )
        
        # In a production environment, this would load the citations into Neo4j
        # For demonstration, we'll simulate the loading
        # loader.load_enriched_citations(resolved_citations, source="gemini_api")
        
        # Simulate loading
        time.sleep(1)
        
        # Update job status
        update_job_status(
            db, 
            job_id, 
            JobStatus.COMPLETED, 
            progress=100.0,
            message=f"Loaded {len(resolved_citations)} opinions into Neo4j"
        )
        
        logger.info(f"Completed Neo4j job {job_id}")
        
    except Exception as e:
        logger.error(f"Error in Neo4j job {job_id}: {str(e)}")
        update_job_status(
            db, 
            job_id, 
            JobStatus.FAILED, 
            error=str(e)
        )

def process_uploaded_csv(db: Session, job_id: int, file_path: str) -> None:
    """
    Process an uploaded CSV file.
    
    Args:
        db: Database session
        job_id: Job ID
        file_path: Path to the uploaded CSV file
    """
    try:
        # Update job status
        update_job_status(
            db, 
            job_id, 
            JobStatus.STARTED, 
            progress=0.0,
            message="Starting CSV processing"
        )
        
        # Load CSV
        update_job_status(
            db, 
            job_id, 
            JobStatus.PROCESSING, 
            progress=10.0,
            message="Loading CSV file"
        )
        
        # In a production environment, this would load the actual CSV file
        # For demonstration, we'll simulate the data
        # df = pd.read_csv(file_path)
        
        # Simulate data
        df = pd.DataFrame({
            "cluster_id": [1001, 1002, 1003],
            "so_plain_text": [
                "This is the text of opinion 1001...",
                "This is the text of opinion 1002...",
                "This is the text of opinion 1003..."
            ]
        })
        
        # Create LLM job
        llm_job_id = create_job(
            db, 
            JobType.LLM_PROCESS, 
            {"file_path": file_path}
        )
        
        # Run LLM job
        run_llm_job(db, llm_job_id, job_id)
        
        # Update job status
        update_job_status(
            db, 
            job_id, 
            JobStatus.COMPLETED, 
            progress=100.0,
            message=f"Processed CSV file with {len(df)} opinions",
            result_path=file_path
        )
        
        logger.info(f"Completed CSV processing job {job_id}")
        
    except Exception as e:
        logger.error(f"Error in CSV processing job {job_id}: {str(e)}")
        update_job_status(
            db, 
            job_id, 
            JobStatus.FAILED, 
            error=str(e)
        )

def run_full_pipeline(
    db: Session,
    neo4j_session,
    extraction_job_id: int,
    llm_job_id: int,
    resolution_job_id: int,
    neo4j_job_id: int,
    config: ExtractionConfig
) -> None:
    """
    Run the full pipeline from extraction to Neo4j loading.
    
    Args:
        db: Database session
        neo4j_session: Neo4j session
        extraction_job_id: ID of the extraction job
        llm_job_id: ID of the LLM job
        resolution_job_id: ID of the resolution job
        neo4j_job_id: ID of the Neo4j job
        config: Extraction configuration
    """
    try:
        # Run extraction job
        run_extraction_job(db, extraction_job_id, config)
        
        # Check if extraction job succeeded
        extraction_job = get_job(db, extraction_job_id)
        if extraction_job["status"] != JobStatus.COMPLETED:
            logger.error(f"Extraction job {extraction_job_id} failed, aborting pipeline")
            return
        
        # Run LLM job
        run_llm_job(db, llm_job_id, extraction_job_id)
        
        # Check if LLM job succeeded
        llm_job = get_job(db, llm_job_id)
        if llm_job["status"] != JobStatus.COMPLETED:
            logger.error(f"LLM job {llm_job_id} failed, aborting pipeline")
            return
        
        # Run resolution job
        run_resolution_job(db, resolution_job_id, llm_job_id)
        
        # Check if resolution job succeeded
        resolution_job = get_job(db, resolution_job_id)
        if resolution_job["status"] != JobStatus.COMPLETED:
            logger.error(f"Resolution job {resolution_job_id} failed, aborting pipeline")
            return
        
        # Run Neo4j job
        run_neo4j_job(db, neo4j_session, neo4j_job_id, resolution_job_id)
        
        logger.info(f"Completed full pipeline")
        
    except Exception as e:
        logger.error(f"Error in full pipeline: {str(e)}")

================
File: src/api/services/stats_service.py
================
from neo4j import GraphDatabase
from typing import List, Dict, Optional, Any
import logging
from datetime import date

from ..models.stats import (
    NetworkStats, 
    CourtStats, 
    TimelineStats,
    YearlyStats,
    TopCitedOpinion,
    TopCitedOpinions
)

logger = logging.getLogger(__name__)

def get_network_stats(neo4j_session) -> NetworkStats:
    """
    Get overall statistics about the citation network.
    
    Args:
        neo4j_session: Neo4j session
        
    Returns:
        Overall network statistics
    """
    query = """
    // Get basic network metrics
    MATCH (o:Opinion)
    OPTIONAL MATCH (o)-[r:CITES]->()
    
    WITH 
        count(DISTINCT o) as total_nodes,
        count(r) as total_edges
    
    // Calculate network density and average degree
    WITH 
        total_nodes,
        total_edges,
        CASE 
            WHEN total_nodes > 1 
            THEN total_edges / (total_nodes * (total_nodes - 1))
            ELSE 0 
        END as network_density,
        CASE 
            WHEN total_nodes > 0 
            THEN toFloat(total_edges) / total_nodes
            ELSE 0 
        END as avg_degree
    
    // Get max degree
    CALL {
        MATCH (o:Opinion)
        OPTIONAL MATCH (o)-[r:CITES]-()
        WITH o, count(r) as degree
        RETURN max(degree) as max_degree
    }
    
    // Get connected components (simplified approximation)
    CALL {
        MATCH (o:Opinion)
        WHERE NOT (o)-[:CITES]-()
        RETURN count(o) as isolated_nodes
    }
    
    // Return network statistics
    RETURN 
        total_nodes,
        total_edges,
        network_density,
        avg_degree,
        max_degree,
        isolated_nodes,
        total_nodes - isolated_nodes as largest_component_size
    """
    
    try:
        # Execute the query
        result = neo4j_session.run(query)
        record = result.single()
        
        if not record:
            # Return default stats if no data
            return NetworkStats(
                total_nodes=0,
                total_edges=0,
                network_density=0.0,
                avg_degree=0.0,
                max_degree=0,
                connected_components=0,
                largest_component_size=0
            )
        
        # Create network stats
        stats = NetworkStats(
            total_nodes=record["total_nodes"],
            total_edges=record["total_edges"],
            network_density=record["network_density"],
            avg_degree=record["avg_degree"],
            max_degree=record["max_degree"],
            # These are approximations since full graph algorithms would be expensive
            connected_components=1 + record["isolated_nodes"],  # Simplified
            largest_component_size=record["largest_component_size"],
            # These would require more complex graph algorithms
            avg_path_length=None,
            diameter=None,
            clustering_coefficient=None,
            metadata={
                "timestamp": date.today().isoformat()
            }
        )
        
        return stats
    
    except Exception as e:
        logger.error(f"Error getting network stats: {str(e)}")
        # Return default stats on error
        return NetworkStats(
            total_nodes=0,
            total_edges=0,
            network_density=0.0,
            avg_degree=0.0,
            max_degree=0,
            connected_components=0,
            largest_component_size=0
        )

def get_court_stats(neo4j_session, limit: int = 10) -> List[CourtStats]:
    """
    Get citation statistics by court.
    
    Args:
        neo4j_session: Neo4j session
        limit: Maximum number of courts to return
        
    Returns:
        List of court statistics
    """
    query = """
    // Get all courts with their opinion counts
    MATCH (o:Opinion)
    WHERE o._court_id IS NOT NULL
    WITH o._court_id as court_id, o.court_name as court_name, count(o) as opinion_count
    ORDER BY opinion_count DESC
    LIMIT $limit
    
    // Get citation counts for each court
    MATCH (o:Opinion)-[r:CITES]->()
    WHERE o._court_id = court_id
    WITH court_id, court_name, opinion_count, count(r) as citation_count
    
    // Get cited-by counts for each court
    MATCH (o:Opinion)<-[r:CITES]-()
    WHERE o._court_id = court_id
    WITH court_id, court_name, opinion_count, citation_count, count(r) as cited_by_count
    
    // Get self-citation counts
    MATCH (o1:Opinion)-[r:CITES]->(o2:Opinion)
    WHERE o1._court_id = court_id AND o2._court_id = court_id
    WITH 
        court_id, 
        court_name, 
        opinion_count, 
        citation_count, 
        cited_by_count,
        count(r) as self_citation_count
    
    // Calculate self-citation ratio
    WITH 
        court_id, 
        court_name, 
        opinion_count, 
        citation_count, 
        cited_by_count,
        CASE 
            WHEN citation_count > 0 
            THEN self_citation_count / toFloat(citation_count)
            ELSE 0 
        END as self_citation_ratio,
        CASE 
            WHEN opinion_count > 0 
            THEN citation_count / toFloat(opinion_count)
            ELSE 0 
        END as avg_citations_per_opinion
    
    // Get top cited courts
    CALL {
        WITH court_id
        MATCH (o1:Opinion)-[r:CITES]->(o2:Opinion)
        WHERE o1._court_id = court_id AND o2._court_id <> court_id
        WITH o2._court_id as cited_court_id, o2.court_name as cited_court_name, count(r) as count
        ORDER BY count DESC
        LIMIT 5
        RETURN collect({court_id: cited_court_id, court_name: cited_court_name, count: count}) as top_cited_courts
    }
    
    // Get top citing courts
    CALL {
        WITH court_id
        MATCH (o1:Opinion)-[r:CITES]->(o2:Opinion)
        WHERE o1._court_id <> court_id AND o2._court_id = court_id
        WITH o1._court_id as citing_court_id, o1.court_name as citing_court_name, count(r) as count
        ORDER BY count DESC
        LIMIT 5
        RETURN collect({court_id: citing_court_id, court_name: citing_court_name, count: count}) as top_citing_courts
    }
    
    // Return court statistics
    RETURN 
        court_id,
        court_name,
        opinion_count,
        citation_count,
        cited_by_count,
        self_citation_ratio,
        avg_citations_per_opinion,
        top_cited_courts,
        top_citing_courts
    ORDER BY opinion_count DESC
    """
    
    try:
        # Execute the query
        result = neo4j_session.run(query, {"limit": limit})
        records = result.records()
        
        court_stats_list = []
        for record in records:
            # Create court stats
            stats = CourtStats(
                court_id=record["court_id"],
                court_name=record["court_name"] or f"Court {record['court_id']}",
                opinion_count=record["opinion_count"],
                citation_count=record["citation_count"],
                cited_by_count=record["cited_by_count"],
                self_citation_ratio=record["self_citation_ratio"],
                avg_citations_per_opinion=record["avg_citations_per_opinion"],
                top_cited_courts=record["top_cited_courts"],
                top_citing_courts=record["top_citing_courts"],
                metadata={
                    "timestamp": date.today().isoformat()
                }
            )
            
            court_stats_list.append(stats)
        
        return court_stats_list
    
    except Exception as e:
        logger.error(f"Error getting court stats: {str(e)}")
        return []

def get_timeline_stats(
    neo4j_session,
    court_id: Optional[str] = None,
    start_year: Optional[int] = None,
    end_year: Optional[int] = None
) -> TimelineStats:
    """
    Get citation statistics over time.
    
    Args:
        neo4j_session: Neo4j session
        court_id: Filter by court ID
        start_year: Start year for timeline
        end_year: End year for timeline
        
    Returns:
        Timeline statistics
    """
    # Build the query based on filters
    filters = []
    params = {}
    
    if court_id:
        filters.append("o._court_id = $court_id")
        params["court_id"] = court_id
    
    filter_clause = " AND ".join(filters)
    if filter_clause:
        filter_clause = f"WHERE {filter_clause}"
    
    query = f"""
    // Get year range
    MATCH (o:Opinion)
    WHERE o.soc_date_filed IS NOT NULL
    {filter_clause}
    WITH 
        min(date.year(o.soc_date_filed)) as min_year,
        max(date.year(o.soc_date_filed)) as max_year
    
    // Apply year range filters if provided
    WITH 
        CASE WHEN $start_year IS NOT NULL AND $start_year > min_year THEN $start_year ELSE min_year END as start_year,
        CASE WHEN $end_year IS NOT NULL AND $end_year < max_year THEN $end_year ELSE max_year END as end_year
    
    // Generate years in range
    UNWIND range(start_year, end_year) as year
    
    // Get opinion counts by year
    CALL {{
        MATCH (o:Opinion)
        WHERE date.year(o.soc_date_filed) = year
        {filter_clause}
        RETURN count(o) as opinion_count
    }}
    
    // Get citation counts by year (outgoing)
    CALL {{
        MATCH (o:Opinion)-[r:CITES]->()
        WHERE date.year(o.soc_date_filed) = year
        {filter_clause}
        RETURN count(r) as citation_count
    }}
    
    // Get cited-by counts by year (incoming, regardless of when the citing opinion was published)
    CALL {{
        MATCH (o:Opinion)<-[r:CITES]-()
        WHERE date.year(o.soc_date_filed) = year
        {filter_clause}
        RETURN count(r) as cited_by_count
    }}
    
    // Calculate average citations per opinion
    WITH 
        year,
        opinion_count,
        citation_count,
        cited_by_count,
        CASE 
            WHEN opinion_count > 0 
            THEN citation_count / toFloat(opinion_count)
            ELSE 0 
        END as avg_citations_per_opinion
    
    // Return yearly statistics
    RETURN 
        collect({{
            year: year,
            opinion_count: opinion_count,
            citation_count: citation_count,
            cited_by_count: cited_by_count,
            avg_citations_per_opinion: avg_citations_per_opinion
        }}) as yearly_stats,
        min(year) as start_year,
        max(year) as end_year,
        count(year) as total_years
    """
    
    try:
        # Execute the query
        result = neo4j_session.run(query, {
            "court_id": court_id,
            "start_year": start_year,
            "end_year": end_year
        })
        record = result.single()
        
        if not record:
            # Return default stats if no data
            return TimelineStats(
                yearly_stats=[],
                total_years=0,
                start_year=start_year or 0,
                end_year=end_year or 0
            )
        
        # Process yearly stats
        yearly_stats = []
        for item in record["yearly_stats"]:
            stats = YearlyStats(
                year=item["year"],
                opinion_count=item["opinion_count"],
                citation_count=item["citation_count"],
                cited_by_count=item["cited_by_count"],
                avg_citations_per_opinion=item["avg_citations_per_opinion"],
                metadata={}
            )
            yearly_stats.append(stats)
        
        # Create timeline stats
        timeline_stats = TimelineStats(
            yearly_stats=yearly_stats,
            total_years=record["total_years"],
            start_year=record["start_year"],
            end_year=record["end_year"]
        )
        
        return timeline_stats
    
    except Exception as e:
        logger.error(f"Error getting timeline stats: {str(e)}")
        # Return default stats on error
        return TimelineStats(
            yearly_stats=[],
            total_years=0,
            start_year=start_year or 0,
            end_year=end_year or 0
        )

def get_top_cited_opinions(
    neo4j_session,
    court_id: Optional[str] = None,
    start_date: Optional[date] = None,
    end_date: Optional[date] = None,
    limit: int = 20
) -> TopCitedOpinions:
    """
    Get the most cited opinions.
    
    Args:
        neo4j_session: Neo4j session
        court_id: Filter by court ID
        start_date: Filter by date range (start)
        end_date: Filter by date range (end)
        limit: Maximum number of opinions to return
        
    Returns:
        List of top cited opinions
    """
    # Build the query based on filters
    filters = []
    params = {"limit": limit}
    
    if court_id:
        filters.append("o._court_id = $court_id")
        params["court_id"] = court_id
    
    if start_date:
        filters.append("o.soc_date_filed >= $start_date")
        params["start_date"] = start_date
    
    if end_date:
        filters.append("o.soc_date_filed <= $end_date")
        params["end_date"] = end_date
    
    filter_clause = " AND ".join(filters)
    if filter_clause:
        filter_clause = f"WHERE {filter_clause}"
    
    query = f"""
    // Get total opinion count
    MATCH (o:Opinion)
    {filter_clause}
    WITH count(o) as total_count
    
    // Get top cited opinions
    MATCH (o:Opinion)<-[r:CITES]-()
    {filter_clause}
    WITH o, count(r) as citation_count, total_count
    ORDER BY citation_count DESC
    LIMIT $limit
    
    // Return top cited opinions
    RETURN 
        collect({{
            cluster_id: o.cluster_id,
            case_name: o.case_name,
            court_id: o._court_id,
            court_name: o.court_name,
            date_filed: o.soc_date_filed,
            citation_count: citation_count
        }}) as opinions,
        total_count
    """
    
    try:
        # Execute the query
        result = neo4j_session.run(query, params)
        record = result.single()
        
        if not record:
            # Return default stats if no data
            return TopCitedOpinions(
                opinions=[],
                total_count=0,
                metadata={}
            )
        
        # Process opinions
        opinions = []
        for item in record["opinions"]:
            opinion = TopCitedOpinion(
                cluster_id=item["cluster_id"],
                case_name=item["case_name"] or f"Opinion {item['cluster_id']}",
                court_id=item["court_id"] or "unknown",
                court_name=item["court_name"] or "Unknown Court",
                date_filed=item["date_filed"],
                citation_count=item["citation_count"],
                metadata={}
            )
            opinions.append(opinion)
        
        # Create top cited opinions
        top_cited = TopCitedOpinions(
            opinions=opinions,
            total_count=record["total_count"],
            metadata={
                "court_id": court_id,
                "start_date": start_date.isoformat() if start_date else None,
                "end_date": end_date.isoformat() if end_date else None,
                "limit": limit
            }
        )
        
        return top_cited
    
    except Exception as e:
        logger.error(f"Error getting top cited opinions: {str(e)}")
        # Return default stats on error
        return TopCitedOpinions(
            opinions=[],
            total_count=0,
            metadata={}
        )

def get_citation_distribution(neo4j_session, bins: int = 10) -> Dict[str, Any]:
    """
    Get the distribution of citation counts.
    
    Args:
        neo4j_session: Neo4j session
        bins: Number of bins for the distribution
        
    Returns:
        Citation count distribution
    """
    query = """
    // Get citation counts for all opinions
    MATCH (o:Opinion)<-[r:CITES]-()
    WITH o, count(r) as citation_count
    
    // Get max citation count for binning
    WITH collect(citation_count) as counts, max(citation_count) as max_count
    
    // Create bins
    WITH counts, max_count, $bins as num_bins
    UNWIND counts as count
    WITH count, max_count, num_bins, floor(count * num_bins / (max_count + 1)) as bin
    
    // Count opinions in each bin
    WITH bin, count(bin) as bin_count, max_count, num_bins
    ORDER BY bin
    
    // Calculate bin ranges
    WITH 
        bin, 
        bin_count, 
        max_count, 
        num_bins,
        floor(bin * (max_count + 1) / num_bins) as bin_start,
        floor((bin + 1) * (max_count + 1) / num_bins) - 1 as bin_end
    
    // Return distribution
    RETURN 
        collect({
            bin: bin,
            bin_start: bin_start,
            bin_end: bin_end,
            count: bin_count,
            label: CASE 
                WHEN bin_start = bin_end THEN toString(bin_start)
                ELSE bin_start + '-' + bin_end
            END
        }) as distribution,
        max_count
    """
    
    try:
        # Execute the query
        result = neo4j_session.run(query, {"bins": bins})
        record = result.single()
        
        if not record:
            # Return default distribution if no data
            return {
                "distribution": [],
                "max_count": 0,
                "bins": bins
            }
        
        # Return distribution
        return {
            "distribution": record["distribution"],
            "max_count": record["max_count"],
            "bins": bins
        }
    
    except Exception as e:
        logger.error(f"Error getting citation distribution: {str(e)}")
        # Return default distribution on error
        return {
            "distribution": [],
            "max_count": 0,
            "bins": bins
        }

================
File: src/api/__init__.py
================
from . import main
from . import database
from . import models
from . import routers
from . import services

__all__ = [
    'main',
    'database',
    'models',
    'routers',
    'services'
]

================
File: src/api/database.py
================
import os
from typing import Generator
from sqlalchemy.orm import Session
from neo4j import GraphDatabase
from dotenv import load_dotenv

# Import database connections from updated modules
from src.postgres.database import get_engine, get_session_factory
from src.neo4j.neomodel_loader import get_neo4j_driver

# Load environment variables
load_dotenv()

# Get existing database connections
engine = get_engine()
SessionLocal = get_session_factory(engine)
neo4j_driver = get_neo4j_driver()

# Dependency to get DB session
def get_db() -> Generator[Session, None, None]:
    """
    Get a PostgreSQL database session.
    
    Yields:
        Session: SQLAlchemy session
    """
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# Dependency to get Neo4j session
def get_neo4j() -> Generator[GraphDatabase.session, None, None]:
    """
    Get a Neo4j database session.
    
    Yields:
        GraphDatabase.session: Neo4j session
    """
    session = neo4j_driver.session()
    try:
        yield session
    finally:
        session.close()

# Verify database connections
def verify_connections() -> dict:
    """
    Verify connections to both databases.
    
    Returns:
        dict: Connection status for each database
    """
    status = {
        "postgresql": False,
        "neo4j": False
    }
    
    # Check PostgreSQL
    try:
        db = SessionLocal()
        db.execute("SELECT 1")
        status["postgresql"] = True
        db.close()
    except Exception as e:
        print(f"PostgreSQL connection error: {e}")
    
    # Check Neo4j
    try:
        with neo4j_driver.session() as session:
            session.run("RETURN 1")
            status["neo4j"] = True
    except Exception as e:
        print(f"Neo4j connection error: {e}")
    
    return status

# Close connections on application shutdown
def close_connections():
    """Close all database connections."""
    neo4j_driver.close()

================
File: src/api/main.py
================
import os
import logging
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configure simple logging
logging.basicConfig(
    level=getattr(logging, os.getenv("LOG_LEVEL", "INFO")),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

# Create FastAPI app
app = FastAPI(
    title="OS Legal Explorer API",
    description="API for exploring legal citation networks",
    version="0.1.0",
)

# Configure CORS - allows all origins for simplicity
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Simple setting for development/hobby project
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Root endpoint
@app.get("/")
async def root():
    return {
        "message": "Welcome to the OS Legal Explorer API",
        "docs": "/docs",
        "version": "0.1.0",
    }

# Health check endpoint
@app.get("/health")
async def health_check():
    from .database import verify_connections
    db_status = verify_connections()
    return {
        "status": "healthy" if all(db_status.values()) else "unhealthy",
        "database": db_status,
    }

# Import and include routers
from .routers import opinions, citations, stats, pipeline
app.include_router(opinions.router)
app.include_router(citations.router)
app.include_router(stats.router)
app.include_router(pipeline.router)

if __name__ == "__main__":
    uvicorn.run(
        "main:app", 
        host="0.0.0.0", 
        port=int(os.getenv("PORT", 8000)),
        reload=True
    )

================
File: src/llm_extraction/models.py
================
from argparse import Action
from pydantic import BaseModel, Field, ConfigDict, field_validator
from enum import StrEnum
from typing import List, Optional, TYPE_CHECKING, ForwardRef
import json
from json_repair import repair_json
from src.postgres.database import find_cluster_id
from datetime import datetime

# Use TYPE_CHECKING to avoid circular imports
if TYPE_CHECKING:
    from src.postgres.models import CitationExtraction, OpinionClusterExtraction
else:
    # Forward references for type hints
    CitationExtraction = ForwardRef('CitationExtraction')
    OpinionClusterExtraction = ForwardRef('OpinionClusterExtraction')

# https://www.law.cornell.edu/citation/6-300
# https://support.vlex.com/how-to/navigate-documents/case-law/treatment-types#undefined
# https://www.lexisnexis.com/pdf/lexis-advance/Shepards-Signal-Indicators-and-analysis-phrases-Final.pdf?srsltid=AfmBOopBSKqOSAAcyYrvQrjavGWoE7ERkWbUrqwM8BrFkYH1RaedBziZ
# https://www.paxton.ai/post/introducing-the-paxton-ai-citator-setting-new-benchmarks-in-legal-research
# from above ^ https://scholarship.law.wm.edu/cgi/viewcontent.cgi?article=1130&context=libpubs; Evaluating Shepard's, KeyCite, and Bcite for Case Validation Accuracy. INACCURATE!

# NOTE to self: Need to think about this model/data in that it's point in time reasoning, Not reasoning today. Need to later combine.


signals = """
(a) Signals that indicate support:
- E.g., : Authority states the proposition with which the citation is associated. Other authorities, not cited, do as well e.g.. "E.g." used with other signals (in which case it is preceded by a comma) similarly indicates the existence of other authorities not cited.
- Accord: Used following citation to authority referred to in text when there are additional authorities that either state or clearly support the proposition with which the citation is associated, but the text quotes only one. Similarly, the law of one jurisdiction may be cited as being in accord with that of another e.g..
- See: Authority supports the proposition with which the citation is associated either implicitly or in the form of dicta e.g. .
- See also: Authority is additional support for the proposition with which the citation is associated (but less direct than that indicated by "see" or "accord"). "See also" is commonly used to refer readers to authorities already cited or discussed e.g.. Generally, it is helpful to include a parenthetical explanation of the source material's relevance following a citation introduced by "see also."
- Cf. : Authority supports by analogy. "Cf." literally means "compare." The citation will only appear relevant to the reader if it is explained. Consequently, in most cases a parenthetical explanations of the analogy should be included e.g..
- Followed by: Authority supports the proposition with which the citation is associated.

(b) Signals that suggest a useful comparison.
- Compare ... with ... : Comparison of authorities that supports proposition. Either side of the comparison can have more than one item linked with "and" e.g.. Generally, a parenthetical explanation of the comparison should be furnished.

(c) Signals that indicate contradiction.
- Contra: Authority directly states the contrary of the proposition with which the citation is associated e.g..
- But see: Authority clearly supports the contrary of the proposition with which citation is associated e.g..
- But cf.: Authority supports the contrary of the position with which the citation is associated by analogy. In most cases a parenthetical explanations of the analogy should be furnished. The word "but" is omitted from the signal when it follows another negative signal e.g..
- Overruled by: The citing case expressly overrules or disapproves all or part of the case cited.
- Abrogated by: The citing case effectively, but not explicitly, overrules or departs from the case cited.
- Superseded by: The citing referencetypically a session law, other record of legislative action or a record of administrative action supersedes the statute, regulation or order cited.

(d) Signals that indicate background material.
- See generally: Authority presents useful background. Parenthetical explanations of the source materials' relevance are generally useful e.g..

(e) Combining a signal with "e.g."
- E.g.,: In addition to the cited authority, there are numerous others that state, support, or contradict the proposition (with the other signal indicating which) but citation to them would not be helpful or necessary. The preceding signal is separated from "e.g." by a comma e.g..
"""


class OpinionSection(StrEnum):
    majority = "MAJORITY"
    concurring = "CONCURRING"
    dissenting = "DISSENTING"


class OpinionType(StrEnum):
    """Type of opinion document."""
    majority = "MAJORITY"
    concurring = "CONCURRING"
    dissenting = "DISSENTING"
    per_curiam = "PER_CURIAM"
    seriatim = "SERIATIM"
    unknown = "UNKNOWN"


class CitationType(StrEnum):
    judicial_opinion = "judicial_opinion"
    statutes_codes_regulations = "statutes_codes_regulations"
    constitution = "constitution"
    administrative_agency_ruling = "administrative_agency_ruling"
    congressional_report = "congressional_report"
    external_submission = "external_submission"
    electronic_resource = "electronic_resource"
    law_review = "law_review"
    legal_dictionary = "legal_dictionary"
    other = "other"


class CitationTreatment(StrEnum):
    positive = "POSITIVE"  # Only when explicitly relied on as key basis
    negative = "NEGATIVE"  # Explicitly disagrees with, distinguishes, or limits
    caution = "CAUTION"  # Expresses doubts or declines to extend
    neutral = "NEUTRAL"  # Default for background/general reference


class Citation(BaseModel):
    """Model for a single citation extracted from a court opinion."""
    page_number: int = Field(
        ...,
        description="The page number where this citation appears (must be a positive integer).",
    )
    citation_text: str = Field(
        ...,
        description="The entirety of the extracted citation text value, resolved back to the original citation when possible.",
    )
    reasoning: str = Field(
        ...,
        description="Detailed analysis explaining this citation's context and relevance (2-4 sentences).",
    )
    type: CitationType = Field(
        ...,
        description="The citation type (e.g., 'judicial_opinion').",
    )
    treatment: CitationTreatment = Field(
        ...,
        description="The citation treatment (POSITIVE, NEGATIVE, CAUTION, or NEUTRAL).",
    )
    relevance: int = Field(
        ...,
        description="Relevance score between 1 (lowest) and 4 (highest).",
    )

    class Config:
        use_enum_values = True

    # Add method to count citation length
    def count_citation_length(self) -> int:
        """Count the length of all string fields in this citation."""
        return len(self.citation_text) + len(self.reasoning)

    @field_validator("type", mode="before")
    @classmethod
    def validate_type(cls, v):
        if isinstance(v, str):
            # Map common input values to enum values
            mapping = {
                "Judicial Opinion": CitationType.judicial_opinion,
                "Statute/Code/Regulation/Rule": CitationType.statutes_codes_regulations,
                "Statute/Code/Regulation": CitationType.statutes_codes_regulations,
                "Constitution": CitationType.constitution,
                "Administrative/Agency Ruling": CitationType.administrative_agency_ruling,
                "Congressional Report": CitationType.congressional_report,
                "External Submission": CitationType.external_submission,
                "Electronic Resource": CitationType.electronic_resource,
                "Law Review": CitationType.law_review,
                "Legal Dictionary": CitationType.legal_dictionary,
                "Other": CitationType.other,
            }

            # Try direct mapping first
            if v in mapping:
                return mapping[v]
            # Try case-insensitive match
            v_lower = v.lower()
            for input_val, enum_val in mapping.items():
                if v_lower == input_val.lower():
                    return enum_val
            # If no match found, try direct enum conversion
            try:
                return CitationType(v)
            except ValueError:
                print(f"Invalid citation type string, defaulting to 'other': {v}")
                return CitationType.other
        elif isinstance(v, CitationType):
            return v
        else:
            print(f"Invalid citation type, defaulting to 'other': {v}")
            return CitationType.other


class CitationAnalysis(BaseModel):
    """Model for a single court opinions complete citation analysis, including the date, brief summary, and citations."""
    date: str = Field(
        ...,
        description="The date of the day the opinion was published, in format YYYY-MM-DD",
    )
    brief_summary: str = Field(
        ..., description="3-5 sentences describing the core holding"
    )
    majority_opinion_citations: List[Citation] = Field(
        ...,
        description="List of citations from the majority opinion, including footnotes.",
    )
    concurring_opinion_citations: List[Citation] = Field(
        ...,
        description="List of citations from concurring opinions, including footnotes.",
    )
    dissenting_citations: List[Citation] = Field(
        ...,
        description="List of citations from dissenting opinions, including footnotes.",
    )

    model_config = ConfigDict(
        json_encoders={str: lambda v: v}  # Preserve Unicode characters in JSON
    )

    # function to count length of all strings in this model
    def count_length(self) -> int:
        sum = 0
        for field in self.model_fields:
            if field.annotation == list or field.annotation == List:
                for item in getattr(self, field.name):
                    if isinstance(item, Citation):
                        sum += item.count_citation_length()
                    # for strings for `notes`
                    elif isinstance(item, str):
                        sum += len(item)
            else:
                sum += len(getattr(self, field.name))
        return sum


class CitationResolved(Citation):
    """This is for the resolved citation, which includes the cluster_id of the resolved citation for case law;"""
    resolved_opinion_cluster: int | None = Field(
        ...,
        description="The cluster_id of the resolved citation for case law; not for statutes or regulations.",
    )
    resolved_statute_code_regulation_rule: int | None = Field(
        ...,
        description="TBD, NEED TO CREATE THIS DATABASE TABLE",
    )


def resolve_citation(citation: Citation) -> CitationResolved:
    """Convert a Citation to a CitationResolved with a postgres db lookup."""
    if citation.type == CitationType.judicial_opinion:
        return CitationResolved(
            **citation.model_dump(),  # Copy all existing fields
            resolved_opinion_cluster=find_cluster_id(citation.citation_text),
            resolved_statute_code_regulation_rule=0,
        )
    else:
        return CitationResolved(
            **citation.model_dump(),
            resolved_opinion_cluster=None,
            resolved_statute_code_regulation_rule=None,
        )


# TODO Overlap with CitationAnalysis.
class CombinedResolvedCitationAnalysis(BaseModel):
    date: str
    cluster_id: int
    brief_summary: str
    majority_opinion_citations: list[CitationResolved]
    concurring_opinion_citations: list[CitationResolved]  # Changed from concurrent to concurring for consistency
    dissenting_citations: list[CitationResolved]

    @classmethod
    def from_citations_json(cls, citations: str, cluster_id: int):
        """Expects a JSON string representing the CitationAnalysis model, from a
        request to Gemini, so checks the `parsed` key."""
        citation_data = None
        if not cluster_id:
            raise ValueError("Cluster ID is required")

        # Handle string input
        if isinstance(citations, str):
            citations = json.loads(citations)

        # If citations is a dict with 'parsed' key, use that
        if isinstance(citations, dict):

            if "parsed" in citations and citations["parsed"] is not None:
                citation_data = citations["parsed"]

                # if something happened with response, and json malformed, try to parse the text
            if isinstance(citation_data, str):
                citation_data = json.loads(citation_data)
        else:
            citation_data = citations

        if citation_data is None:
            # try and parse the text
            try:
                citation_data = citations["candidates"][0]["content"]["parts"][0][
                    "text"
                ]

                citation_data = repair_json(citation_data)
                # step 3, load into json
                citation_data = json.loads(citation_data)

            except Exception as e:
                print("Could not parse citation for cluster_id: %s, %s", cluster_id, e)
                citation_data = None

        # print(type(citation_data))
        # print(citation_data)
        # Convert to CitationAnalysis model and use existing from_citations method
        majority_opinion_citations = []
        concurring_opinion_citations = []
        dissenting_citations = []
        keys = [
            "majority_opinion_citations",
            "concurring_opinion_citations",
            "dissenting_citations",
        ]
        # if not all keys are present, only lookup found keys
        if not all(key in citation_data.keys() for key in keys):
            keys = [key for key in keys if key in citation_data.keys()]

        for key in keys:
            for citation in citation_data[key]:
                # compare dictionary keys of citation with Citation model
                if not all(
                    field in citation.keys() for field in Citation.model_fields.keys()
                ):
                    continue
                else:
                    Citation(
                        citation_text=citation["citation_text"],
                        reasoning=citation["reasoning"],
                        type=citation["type"],
                        treatment=citation["treatment"],
                        relevance=citation["relevance"],
                    )
                    if key == "majority_opinion_citations":
                        majority_opinion_citations.append(citation)
                    elif key == "concurring_opinion_citations":
                        concurring_opinion_citations.append(citation)
                    elif key == "dissenting_citations":
                        dissenting_citations.append(citation)
        citation_analysis = CitationAnalysis(
            date=citation_data["date"],
            brief_summary=citation_data["brief_summary"],
            majority_opinion_citations=majority_opinion_citations,
            concurring_opinion_citations=concurring_opinion_citations,
            dissenting_citations=dissenting_citations,
        )

        # citation_analysis = CitationAnalysis.model_validate(citation_data)
        return cls.from_citations([citation_analysis], cluster_id)

    @classmethod
    def from_citations(
        cls,
        citations: list[CitationAnalysis],
        cluster_id: int,
    ) -> "CombinedResolvedCitationAnalysis":

        return cls(
            date=citations[0].date,
            cluster_id=cluster_id,
            brief_summary=citations[0].brief_summary,
            majority_opinion_citations=[
                resolve_citation(citation)
                for citation_analysis in citations
                for citation in citation_analysis.majority_opinion_citations
            ],
            concurring_opinion_citations=[
                resolve_citation(citation)
                for citation_analysis in citations
                for citation in citation_analysis.concurring_opinion_citations
            ],
            dissenting_citations=[
                resolve_citation(citation)
                for citation_analysis in citations
                for citation in citation_analysis.dissenting_citations
            ],
        )

    def get_opinion_nodes(self) -> dict:
        """Return a dictionary of unique opinion nodes needed for this citation analysis.
        The key is the opinion cluster_id and the value is a dict of properties.
        For the citing opinion, includes 'date_filed'; for others, only 'cluster_id'."""
        nodes = {}
        # Add the citing opinion node
        nodes[self.cluster_id] = {
            "cluster_id": self.cluster_id,
            "date_filed": self.date,
            "type": "judicial_opinion",
        }

        # Helper function to process a list of citations
        def process_citations(citations):
            for citation in citations:
                if citation.resolved_opinion_cluster is not None:
                    if citation.resolved_opinion_cluster not in nodes:
                        nodes[citation.resolved_opinion_cluster] = {
                            "cluster_id": citation.resolved_opinion_cluster,
                            "type": citation.type,  # Add citation type from the model
                        }

        process_citations(self.majority_opinion_citations)
        process_citations(self.concurring_opinion_citations)
        process_citations(self.dissenting_citations)

        return nodes


def to_sql_models(
    combined: CombinedResolvedCitationAnalysis,
) -> tuple[OpinionClusterExtraction, list[CitationExtraction]]:
    """Convert a CombinedResolvedCitationAnalysis to SQL models.

    Returns:
        Tuple containing:
        - OpinionClusterExtraction instance
        - List of CitationExtraction instances
    """
    # Create the opinion cluster extraction
    cluster = OpinionClusterExtraction(
        cluster_id=combined.cluster_id,
        date_filed=datetime.strptime(combined.date, "%Y-%m-%d"),
        brief_summary=combined.brief_summary,
    )

    citations = []

    # Helper to process citations from a section
    def process_section_citations(citation_list: list[CitationResolved], section: OpinionSection):
        for citation in citation_list:
            cite = CitationExtraction(
                opinion_cluster_extraction_id=cluster.id,  # This will be set after DB insert
                section=section,
                citation_type=citation.type,
                citation_text=citation.citation_text,
                page_number=citation.page_number,
                treatment=citation.treatment,
                relevance=citation.relevance,
                reasoning=citation.reasoning,
                resolved_opinion_cluster=citation.resolved_opinion_cluster,
                resolved_text=citation.citation_text,  # Using original text as resolved for now
            )
            citations.append(cite)

    # Process each section
    process_section_citations(
        combined.majority_opinion_citations, OpinionSection.majority
    )
    process_section_citations(
        combined.concurring_opinion_citations, OpinionSection.concurring
    )
    process_section_citations(
        combined.dissenting_citations, OpinionSection.dissenting
    )

    return cluster, citations


def from_sql_models(
    cluster: OpinionClusterExtraction,
) -> CombinedResolvedCitationAnalysis:
    """Convert SQL models back to a CombinedResolvedCitationAnalysis.

    Args:
        cluster: OpinionClusterExtraction instance with citations relationship loaded
    """
    # Sort citations by section
    majority_citations = []
    concurring_citations = []
    dissenting_citations = []

    for citation in cluster.citations:
        resolved_citation = CitationResolved(
            page_number=citation.page_number or 1,  # Default to 1 if None
            citation_text=citation.citation_text,
            reasoning=citation.reasoning or "",  # Default to empty string if None
            type=citation.citation_type,
            treatment=citation.treatment
            or CitationTreatment.neutral,  # Default to neutral if None
            relevance=citation.relevance or 1,  # Default to 1 if None
            resolved_opinion_cluster=citation.resolved_opinion_cluster,
            resolved_statute_code_regulation_rule=None,  # Not implemented yet
        )

        if citation.section == OpinionSection.majority:
            majority_citations.append(resolved_citation)
        elif citation.section == OpinionSection.concurring:
            concurring_citations.append(resolved_citation)
        elif citation.section == OpinionSection.dissenting:
            dissenting_citations.append(resolved_citation)

    return CombinedResolvedCitationAnalysis(
        date=cluster.date_filed.strftime("%Y-%m-%d"),
        cluster_id=cluster.cluster_id,
        brief_summary=cluster.brief_summary,
        majority_opinion_citations=majority_citations,
        concurring_opinion_citations=concurring_citations,
        dissenting_citations=dissenting_citations,
    )

================
File: src/llm_extraction/prompts.py
================
system_prompt = """
You are a legal expert analyzing court opinions, specifically focusing on citation analysis. Your work will help democratize access to legal knowledge and create more accurate and equitable legal research tools for lawyers, scholars, and the public. This analysis will help build the next generation of legal AI systems that can make justice more accessible to all.

## CITATION TYPES TO ANALYZE:
- Judicial Opinion
- Statute/Code/Regulation/Rule 
- Constitution
- Administrative/Agency Ruling
- Congressional Report
- External Submission
- Electronic Resource
- Law Review
- Legal Dictionary
- Other (For less common legal citations; can include things like treatises, restatements, etc.  If you are unsure, categorize as "Other".)

## CITATIONS TO IGNORE:
- Affadavits: (These are evidentiary documents, not legal authority being analyzed.)

## CITATION TREATMENT CRITERIA:
- POSITIVE: Use only when the court explicitly relies on and affirms the citation's ruling as a key basis for its decision. The citation must be central to the court's reasoning, not merely supportive. (Use rarely)
- NEGATIVE: Use when the court explicitly disagrees with, distinguishes, limits, or overrules ANY part of the citation's application. (Use rarely)
- CAUTION: Use when the court expresses doubts, declines to extend the ruling, or finds the citation only partially applicable, or the cited case is is used to show dissimilarity to the case at hand.
- NEUTRAL: The default treatment for background, comparison, or general reference.

## CITATION RELEVANCE SCALE (Use a whole number scale; Be very conservative):
- 1: Passing reference or background information
- 2: Supports a minor point or provides context
- 3: Important to a specific argument or sub-issue
- 4: Absolutely central to the core holding (use rarely).

## SPECIAL HANDLING:
- Analyze citations from dissenting or concurring opinions separately, and group them into their respective lists.
- For 'Id.' citations and 'supra' references and other citations that are not full citations: Resolve to the original citation within the same opinion section (majority, concurring, or dissenting) and ensure consistent treatment
- Each citation MUST be analyzed individually, even if multiple citations appear in the same paragraph. Do not group separate citations together in your analysis.
- Only analyze citations that explicitly appear in the source text; do not infer or add citations that aren't present.
- Group footnote citations with the citations from the same opinion (e.g., majority opinion footnotes should be grouped with majority opinion citations)
- Do NOT attribute citations to concurring/dissenting opinions unless explicitly stated in text (e.g., "Justice Smith, dissenting, wrote...").
- The text may include repeated headers and footers from PDF conversion that are not part of the main content. Please ignore these extraneous elements and join fragmented sentences from body paragraphs and footnotes across page breaks to maintain coherent reasoning and accurate citation extraction.
- If it is a long case, focus the analysis and extraction on the majority opinion.

## REQUIRED OUTPUT FORMAT IN JSON:
Your JSON must adhere to the provided schema exactly. The output should include:
- `date`: The date of the day the opinion was published, in format YYYY-MM-DD.
- `brief_summary`: 35 sentences describing the core holding.
- `majority_opinion_citations`, `concurrent_opinion_citations`, `dissenting_citations`: Lists of citations, each with:
  - `page_number`: The page number where the citation appears.
  - `citation_text`: The full citation text.
  - `reasoning`: 2-4 sentences explaining this specific citation's use in context and its relevance.
  - `type`: [Citation Type, e.g., "judicial_opinion"]
  - `treatment`: [POSITIVE/NEGATIVE/CAUTION/NEUTRAL]
  - `relevance`: [14]

Your careful analysis of each individual citation will help build more accurate and equitable legal research tools that can serve justice worldwide.
"""
parahrapg = "- `paragraph_number`: [Paragraph number where the citation appears]"
backup_schema_prompt = """
REQUIRED OUTPUT FORMAT IN JSON:
A JSON Schema will be provided to you to ensure the output is valid.

Brief Summary: [3-5 sentences describing the core holding]
Date: [Date of the opinion]

GROUP CITATIONS BY TYPE:
- majority_opinion_citations (MAJORITY / PER CURIAM / CONSENSUS OPINION)
- dissenting_citations (DISSENTING OPINION)
- concurrent_opinion_citations (CONCURRENT OPINIONS)
- other_citations (OTHER)

For each group, provide:
Citations Analysis:
[Paragraph Number]:
    [Single Citation]:
        Type: [Citation Type]
        Treatment: [POSITIVE/NEGATIVE/CAUTION/NEUTRAL]
        Relevance: [1-4]
        Summary and Reasoning: [3-4 sentences explaining this specific citation's use in context and its relevance.]

Note: For paragraphs with multiple citations, create separate entries for each citation, like:
5:
    Citation "Smith v. Jones":
        [Analysis...]
5:
    Citation "Brown v. Wilson":
        [Analysis...]
"""


system_prompt_legacy = """
You are a legal expert analyzing court opinions, specifically focusing on citation analysis. Your work will help democratize access to legal knowledge and create more accurate and equitable legal research tools for lawyers, scholars, and the public. This analysis will help build the next generation of legal AI systems that can make justice more accessible to all.

## CITATION TYPES TO ANALYZE:
- Judicial Opinion
- Statute/Code/Regulation/Rule 
- Constitution
- Administrative/Agency Ruling
- Congressional Report
- External Submission
- Electronic Resource
- Law Review
- Legal Dictionary
- Other (For less common legal citations; can include things like treatises, restatements, etc.  If you are unsure, categorize as "Other".)

## CITATIONS TO IGNORE:
- Affadavits: (These are evidentiary documents, not legal authority being analyzed.)

## CITATION TREATMENT CRITERIA:
- POSITIVE: Use when the court explicitly relies on and affirms the citation's ruling as a key basis for its decision. The citation is central to the court's reasoning, not merely supportive. Aim to use this when the cited case is foundational to the current court's decision, only a few per opinion maximum.
- NEGATIVE: Use when the court explicitly disagrees with, distinguishes, limits, or overrules any part of the citation's application.  This indicates the court is actively pushing back against the cited authority.
- CAUTION: Use when the court expresses doubts, declines to extend the ruling, or finds the citation only partially applicable, or the cited case is used to show dissimilarity to the case at hand. This is for citations treated with some reservation or qualification.
- NEUTRAL: The default treatment for background, comparison, or general reference. Use when the citation provides context or is mentioned without significant impact on the court's core reasoning.

## CITATION RELEVANCE SCALE (Use a whole number scale):
- 1: Passing reference or very general background information. Barely relevant to the immediate legal issue.
- 2: Provides context or supports a minor, non-essential point. Contributes to background understanding but isn't crucial to the core argument. Would be able to argue the opinion without the citation.
- 3: Important to a specific argument or sub-issue. Directly supports a key step in the court's reasoning or addresses a significant aspect of the legal question.
- 4: Absolutely central to the core holding. The cited material is foundational to the court's ultimate decision. Reserve this for citations that are indispensable to understanding the court's judgment. Almost necessary to argue the opinion.

## OPINION TYPE:
- MAJORITY: Use when the citation is included in the majority opinion.
- CONCURRING: Use when the citation is included in a concurring opinion.
- DISSENTING: Use when the citation is included in a dissenting opinion.

## SPECIAL HANDLING:
- For 'Id.' citations and 'supra' references and other citations that are not full citations: Resolve to the original citation within the same opinion section (majority, concurring, or dissenting) and ensure consistent treatment
- Each citation MUST be analyzed individually, even if multiple citations appear in the same paragraph. Do not group separate citations together in your analysis.
- Do not miss any citations, but only ones that explicitly appear in the source text; do not infer or add citations that aren't present.
- Analyze each citation on every paragraph it appears. Ensure you make an entry for each paragraph if a citation spans multiple paragraphs.
- Group footnote citations with the citations from the same opinion (e.g., majority opinion footnotes should be grouped with majority opinion citations)
- Do NOT attribute citations to concurring/dissenting opinions unless explicitly stated in text (e.g., "Justice Smith, dissenting, wrote...").
- The text may include repeated headers and footers from PDF conversion that are not part of the main content. Please ignore these extraneous elements and join fragmented sentences from body paragraphs and footnotes across page breaks to maintain coherent reasoning and accurate citation extraction.

## REQUIRED OUTPUT FORMAT IN JSON:
Your JSON must adhere to the provided schema exactly. The output should include:
- `date`: The date of the day the opinion was published, in format YYYY-MM-DD.
- `brief_summary`: 35 sentences describing the core holding.
- `legal_issue`: 1-2 sentences posing a legal question to be addressed by the court.
- `citations`: Lists of citations, each with:
  - `citation_text`: The full citation text.
  - `paragraph_number`: [Paragraph number where the citation appears]
  - `reasoning`: 2-4 sentences explaining this specific citation's use in context and its relevance.
  - `type`: [Citation Type, e.g., "judicial_opinion"]
  - `treatment`: [POSITIVE/NEGATIVE/CAUTION/NEUTRAL]
  - `relevance`: [14]
  - `opinion_type`: [MAJORITY/CONCURRING/DISSENTING]

Your careful analysis of each individual citation will help build more accurate and equitable legal research tools that can serve justice worldwide.
"""

structured_output_prompt = """
Please convert the following citation analysis into a structured JSON format that matches these Pydantic models:

CitationAnalysis:
- brief_summary (str): Extract the content under "Brief Summary:" 
- citations (List[Citation]): Array of Citation objects

Citation:
- text (str): The exact citation text
- type (CitationType): One of [electronic_resource, judicial_opinion, constitution, statutes_codes_regulations, arbitration, court_rules, books, law_journal, other]
- treatment (CitationTreatment): One of [POSITIVE, NEGATIVE, CAUTION, NEUTRAL]
- relevance (int): The number from 1-4 listed under "Relevance"
- reasoning (str): The explanation under "Reasoning"
- paragraph_number (int): The paragraph number where citation appears
- is_dissenting_opinion (bool): Whether citation is from dissenting opinion
- other (str): Any additional notes or context not covered by other fields

Please ensure:
1. All citations are properly structured, including those from dissenting opinions
2. The "Type", "Treatment", and "Relevance" values exactly match the enumerated options
3. Paragraph numbers are integers
4. The output is valid JSON that can be parsed by the Pydantic models

Example desired output format:
{
  "brief_summary": "This court opinion upheld the lower court's....",
  "citations": [
    {
      "text": "U.S. Const. art. III,  2, cl. 2.",
      "type": "constitution",
      "treatment": "NEUTRAL", 
      "relevance": 2,
      "reasoning": "The court mentioned the constitution in the context of the case.",
      "paragraph_number": 1,
      "is_dissenting_opinion": false,
      "other": "string"
    }
  ]
}
"""

================
File: src/llm_extraction/rate_limited_gemini.py
================
import time
import json
from typing import Any, Dict, List, Optional, Union
from google import genai
from google.genai.types import GenerateContentConfig, GenerateContentResponse
from google.genai.chats import Chat
from json_repair import repair_json
import os
import pandas as pd
import logging
from concurrent.futures import ThreadPoolExecutor
from threading import Lock, Semaphore
from tqdm import tqdm
from datetime import datetime
from threading import local

from src.llm_extraction.models import CitationAnalysis
from src.llm_extraction.prompts import system_prompt

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


class TextChunker:
    """Handles text chunking and response combining."""

    @staticmethod
    def count_words(text: str) -> int:
        """Count words in text, ignoring empty lines and extra whitespace."""
        return len([word for word in text.strip().split() if word])

    @staticmethod
    def split_paragraphs(text: str) -> List[str]:
        """Split text into paragraphs, handling various newline formats."""
        # Handle different types of paragraph separators
        text = text.replace("\r\n", "\n")  # Normalize line endings

        # Split on double newlines (common paragraph separator)
        paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]

        # If no paragraphs found with double newlines, try single newlines
        if len(paragraphs) <= 1:
            paragraphs = [p.strip() for p in text.split("\n") if p.strip()]

        if not paragraphs:
            logging.warning(
                f"No paragraphs found in text, returning text as single chunk: {text[0:50]}..."
            )
            return [text]

        return paragraphs

    @staticmethod
    def simple_split_into_chunks(text: str, max_words: int = 10000) -> List[str]:
        """Split text into chunks using paragraph boundaries so that each chunk is under max_words."""
        paragraphs = TextChunker.split_paragraphs(text)
        chunks = []
        current_chunk = []
        current_word_count = 0

        for paragraph in paragraphs:
            paragraph_word_count = TextChunker.count_words(paragraph)

            # Handle paragraphs that are themselves too long
            if paragraph_word_count > max_words:
                # First flush any current chunk
                if current_chunk:
                    chunks.append("\n\n".join(current_chunk))
                    current_chunk = []
                    current_word_count = 0

                # Split the long paragraph on word boundaries
                words = paragraph.split()
                temp_words = []
                temp_count = 0

                for word in words:
                    if temp_count + 1 > max_words:
                        chunks.append(" ".join(temp_words))
                        temp_words = [word]
                        temp_count = 1
                    else:
                        temp_words.append(word)
                        temp_count += 1

                if temp_words:
                    current_chunk = [" ".join(temp_words)]
                    current_word_count = temp_count
                continue

            # Normal case: If adding the paragraph would exceed max_words, start new chunk
            if current_word_count + paragraph_word_count > max_words:
                if current_chunk:
                    chunks.append("\n\n".join(current_chunk))
                current_chunk = [paragraph]
                current_word_count = paragraph_word_count
            else:
                current_chunk.append(paragraph)
                current_word_count += paragraph_word_count

        if current_chunk:
            chunks.append("\n\n".join(current_chunk))

        return chunks


class TokenBucket:
    """Token bucket algorithm for rate limiting."""

    def __init__(self, rate: int, capacity: int):
        self.rate = rate  # tokens per minute
        self.capacity = capacity
        self.tokens = capacity
        self.last_update = time.time()
        self.lock = Lock()

    def _add_tokens(self) -> None:
        """Add tokens based on elapsed time."""
        now = time.time()
        elapsed_minutes = (now - self.last_update) / 60.0  # Convert to minutes
        new_tokens = elapsed_minutes * self.rate  # Rate is already in tokens per minute
        self.tokens = min(self.capacity, self.tokens + new_tokens)
        self.last_update = now

    def try_acquire(self) -> bool:
        """Try to acquire a token without blocking."""
        with self.lock:
            self._add_tokens()
            if self.tokens >= 1:
                self.tokens -= 1
                return True
            return False

    def acquire(self) -> None:
        """Acquire a token, blocking if necessary."""
        while True:
            with self.lock:
                self._add_tokens()
                if self.tokens >= 1:
                    self.tokens -= 1
                    return
            time.sleep(0.1)  # Sleep outside the lock


class RateLimiter:
    """Rate limiter using token bucket algorithm with concurrent request limiting."""

    def __init__(self, rpm_limit: int = 15, max_concurrent: int = 10):
        self.token_bucket = TokenBucket(rate=rpm_limit, capacity=rpm_limit)
        self.concurrent_semaphore = Semaphore(max_concurrent)

    def acquire(self) -> None:
        """Acquire both rate limit and concurrency permits."""
        self.token_bucket.acquire()
        self.concurrent_semaphore.acquire()

    def release(self) -> None:
        """Release concurrency permit."""
        self.concurrent_semaphore.release()


class ResponseSerializer:
    """Handles serialization of Gemini API responses."""

    @staticmethod
    def serialize(response: GenerateContentResponse | None) -> Dict[str, Any]:
        """Serialize a response to a consistent dictionary format."""
        if response is None:
            raise ValueError("Invalid input type")

        if response.parsed:
            # doubt this will be the case, think OpenAI client does this casting automatically.
            if isinstance(response.parsed, CitationAnalysis):
                return response.parsed.model_dump()

            if isinstance(response.parsed, str):
                try:
                    return json.loads(response.parsed)
                except json.JSONDecodeError:
                    raise ValueError("Failed to parse response as JSON")
            if isinstance(response.parsed, dict):
                return response.parsed

            raise ValueError(
                f"Unexpected response type for response.parsed: {type(response.parsed)}"
            )

        # try and decode from `text` field, which may be invalid json
        if response.text is not None:
            try:
                return json.loads(response.text)
            except json.JSONDecodeError:
                try:
                    return repair_json(response.text)
                except Exception as e:
                    logging.error(f"Failed to parse response as JSON: {str(e)}")
                    return {"error": str(e)}

        return {"error": "No response data available"}


class GeminiClient:
    DEFAULT_MODEL = "gemini-2.0-flash-001"

    def __init__(
        self,
        api_key: str,
        rpm_limit: int = 15,
        max_concurrent: int = 10,
        config: Optional[GenerateContentConfig] = None,
        model: str = DEFAULT_MODEL,
    ):
        self.client = genai.Client(api_key=api_key)
        self.config = config
        self.rate_limiter = RateLimiter(
            rpm_limit=rpm_limit, max_concurrent=max_concurrent
        )
        self.chunker = TextChunker()
        self.serializer = ResponseSerializer()
        self.model = model

        # Initialize thread-local storage for worker IDs
        self.worker_data = local()
        self.worker_counter = 0
        self.worker_counter_lock = Lock()

        logging.info(
            f"Initialized client with RPM limit: {rpm_limit}, AFC concurrent limit: {max_concurrent}, model: {model}"
        )

    def get_worker_id(self) -> int:
        """Get or create worker ID for current thread."""
        # Make the entire check-and-assign operation atomic
        with self.worker_counter_lock:
            if not hasattr(self.worker_data, "worker_id"):
                self.worker_counter += 1
                self.worker_data.worker_id = self.worker_counter
        return self.worker_data.worker_id

    def generate_content_with_chat(
        self, text: str, model: str = "gemini-2.0-flash-001"
    ) -> Union[List[Dict]]:
        """Generate content using chat history for better context preservation."""
        model = model or self.model
        worker_id = self.get_worker_id()

        word_count = self.chunker.count_words(text)

        # Create a copy of the config with chunking instructions if needed
        if word_count > 10000:
            config = GenerateContentConfig(
                response_mime_type=self.config.response_mime_type,
                response_schema=self.config.response_schema,
                system_instruction=(
                    self.config.system_instruction
                    + "\n\n The document will be sent in multiple parts. "
                    "For each part, analyze the citations and legal arguments while maintaining context "
                    "from previous parts. Please provide your analysis in the same structured format "
                    "filling in the lists of citation analysis for each response."
                ),
            )
        else:
            config = self.config

        if word_count <= 10000:
            try:
                self.rate_limiter.acquire()
                response: GenerateContentResponse | None = (
                    self.client.models.generate_content(
                        model=model,
                        config=config,
                        contents=text.strip(),
                    )
                )
                if not response:
                    raise ValueError(
                        f"Received empty response from API for text of length {word_count}"
                    )
                # Use the serializer to convert response to a proper dict
                return [self.serializer.serialize(response)]
            except Exception as e:
                logging.error(
                    f"Worker {worker_id}: API call failed for text of length {word_count}: {str(e)}"
                )
                raise
            finally:
                self.rate_limiter.release()

        logging.info(
            f"Worker {worker_id}: Text length ({word_count} words) exceeds limit. Using chat-based chunking..."
        )
        chunks = self.chunker.simple_split_into_chunks(text)
        chat = None

        try:
            # Create a chat session with chunked config
            chat: Chat = self.client.chats.create(model=model, config=config)
            responses: List[Dict] = []
            for i, chunk in enumerate(chunks, 1):
                chunk_words = self.chunker.count_words(chunk)
                logging.info(
                    f"Worker {worker_id}: Processing chunk {i}/{len(chunks)} ({chunk_words} words)"
                )

                self.rate_limiter.acquire()
                try:
                    response: GenerateContentResponse = chat.send_message(chunk.strip())
                    if not response or not hasattr(response, "text"):
                        raise ValueError(
                            f"Invalid or empty response from chunk {i} (length: {chunk_words})"
                        )
                    responses.append(self.serializer.serialize(response))
                except Exception as e:
                    logging.error(
                        f"Worker {worker_id}: Failed to process chunk {i}/{len(chunks)} "
                        f"(length: {chunk_words}): {str(e)}"
                    )
                    raise
                finally:
                    self.rate_limiter.release()

            if not responses:
                raise ValueError("No valid responses were collected during processing")

            return responses

        except Exception as e:
            logging.error(
                f"Worker {worker_id}: Chat-based processing failed for text of length {word_count}: {str(e)}"
            )
            raise
        finally:
            if chat:
                try:
                    del chat
                except Exception as e:
                    logging.warning(f"Failed to cleanup chat session: {str(e)}")

    def process_dataframe(
        self,
        df: pd.DataFrame,
        text_column: str = "text",
        model: str = "gemini-2.0-flash-001",
        max_workers: Optional[int] = None,
        output_file: Optional[str] = None,
        batch_size: int = 10,
    ) -> Dict[Any, Any]:
        """Process a DataFrame of content generation requests using thread pool."""
        if max_workers is None:
            max_workers = min(
                10, self.rate_limiter.token_bucket.rate
            )  # Conservative default

        results = {}
        errors = []
        total_processed = 0

        def process_row(row) -> tuple[str, List[Dict], Optional[str]]:
            worker_id = self.get_worker_id()
            try:
                logging.info(
                    f"Worker {worker_id}: Starting processing cluster_id {row['cluster_id']}"
                )
                result = self.generate_content_with_chat(row[text_column], model)
                logging.info(
                    f"Worker {worker_id}: Finished processing cluster_id {row['cluster_id']}"
                )
                return row["cluster_id"], result, None
            except Exception as e:
                logging.error(
                    f"Worker {worker_id}: Error processing cluster_id {row['cluster_id']}: {str(e)}"
                )
                return row["cluster_id"], None, str(e)

        logging.info(
            f"Processing {len(df)} rows with {max_workers} workers in batches of {batch_size}"
        )

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Process in batches
            for batch_start in range(0, len(df), batch_size):
                batch_end = min(batch_start + batch_size, len(df))
                batch_df = df.iloc[batch_start:batch_end]

                logging.info(
                    f"Processing batch {batch_start//batch_size + 1}, rows {batch_start} to {batch_end}"
                )

                # Create futures for this batch only
                futures = [
                    executor.submit(process_row, row) for _, row in batch_df.iterrows()
                ]

                # Process futures for this batch
                for future in tqdm(
                    futures,
                    total=len(futures),
                    desc=f"Processing batch {batch_start//batch_size + 1}",
                    position=0,
                    leave=True,
                ):
                    try:
                        cluster_id, result, error = future.result()
                        results[cluster_id] = result
                        total_processed += 1

                        if error:
                            errors.append(
                                {"cluster_id": str(cluster_id), "error": error}
                            )

                        if output_file:
                            # Use atomic write to prevent corruption
                            tmp_file = f"{output_file}.tmp"
                            with open(tmp_file, "w") as f:
                                json.dump(
                                    {str(cid): resp for cid, resp in results.items()},
                                    f,
                                )
                            os.replace(tmp_file, output_file)

                    except Exception as e:
                        worker_id = self.get_worker_id()
                        logging.error(
                            f"Worker {worker_id}: Failed to process future in batch {batch_start//batch_size + 1}: {str(e)}"
                        )

                # Clear the futures list after batch is done
                futures.clear()

        if errors:
            logging.warning(f"Encountered {len(errors)} errors during processing")

        return results


# Example usage:
def main():
    config = GenerateContentConfig(
        response_mime_type="application/json",
        response_schema=CitationAnalysis,
        system_instruction=system_prompt,
    )

    client = GeminiClient(
        api_key=os.getenv("GEMINI_API_KEY"),
        rpm_limit=10,  # Conservative limits for testing
        max_concurrent=10,  # Respect AFC limit
        config=config,
    )

    df = pd.read_csv("data_final/supreme_court_1950_some_processing.csv")
    df = df.sample(250)  # Take first 25 rows for testing

    # Process DataFrame with max_workers respecting AFC limit
    results = client.process_dataframe(
        df,
        text_column="text",
        output_file=f"responses_trial_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        max_workers=10,  # Match AFC limit
    )

    print(f"Processed {len(results)} items")


if __name__ == "__main__":
    main()

================
File: src/neo4j/models.py
================
from neomodel import (
    StructuredNode,
    StructuredRel,
    StringProperty,
    IntegerProperty,
    DateTimeProperty,
    DateProperty,
    RelationshipTo,
    RelationshipFrom,
    BooleanProperty,
    ArrayProperty,
    UniqueIdProperty,
    JSONProperty,
    ZeroOrMore,
    db,
)
from datetime import datetime
from src.llm_extraction.models import CitationType, CitationTreatment, OpinionType
import asyncio


class CitesRel(StructuredRel):
    """
    Represents a citation relationship between opinions with enriched metadata.
    All fields except source are optional to accommodate different data sources.
    """

    # Required fields
    source = StringProperty(required=True)  # Source dataset identifier
    version = IntegerProperty(default=1)

    # Metadata fields - index only treatment as it's commonly queried
    treatment = StringProperty(
        choices=CitationTreatment, index=True, default=None
    )  # Frequently filtered
    relevance = IntegerProperty(default=None)
    reasoning = StringProperty(default=None)
    citation_text = StringProperty(default=None)
    page_number = IntegerProperty(default=None)

    # Temporal fields
    timestamp = DateTimeProperty(default=lambda: datetime.now())
    opinion_section = StringProperty(default=None)  # majority, dissent, or concurrent

    # Audit trail
    other_metadata_versions = JSONProperty(default=list)


class BaseNode(StructuredNode):
    """Base class for all nodes with common properties and methods."""

    # Common timestamps for all nodes
    created_at = DateTimeProperty(default=lambda: datetime.now())
    updated_at = DateTimeProperty(default=lambda: datetime.now())

    # Abstract base class
    __abstract_node__ = True

    def pre_save(self):
        """Update timestamp before saving"""
        self.updated_at = datetime.now()


class Citation(BaseNode):
    """
    Base citation node for all types of citations.
    The type field determines how the citation should be interpreted.
    Additional metadata can be stored in the metadata JSON field.
    """

    # Core fields with indexes
    citation_text = StringProperty(unique_index=True, required=True)
    type = StringProperty(required=True, choices=CitationType, index=True)
    metadata = JSONProperty(default=dict)

    # Relationships
    cited_by = RelationshipFrom(
        "Citation", "CITES", model=CitesRel, cardinality=ZeroOrMore
    )
    cites = RelationshipTo("Citation", "CITES", model=CitesRel, cardinality=ZeroOrMore)

    @property
    def citation_count(self):
        """Get real-time citation count from relationships"""
        return len(self.cited_by.all())

    @classmethod
    def get_citations_by_type(cls, citation_type: CitationType):
        """Get all citations of a specific type"""
        return cls.nodes.filter(type=citation_type)


class Opinion(Citation):
    """
    Specialized Citation node representing judicial opinions.
    Inherits base citation functionality and adds opinion-specific fields.
    
    IMPORTANT: The full text of opinions should NOT be stored in Neo4j.
    Original opinion text is stored in the PostgreSQL database in the opinion_text table.
    Neo4j should only contain metadata and citation relationships for efficient graph traversal.
    """

    # Set the type automatically for all Opinion instances
    type = StringProperty(default=CitationType.judicial_opinion, required=True)

    def pre_save(self):
        """Ensure opinion type is always correct"""
        super().pre_save()
        if self.type != CitationType.judicial_opinion:
            raise ValueError("Opinions must have type CitationType.judicial_opinion")

    # Primary identifier
    cluster_id = IntegerProperty(unique_index=True, required=True)

    # Core metadata - index frequently queried fields
    date_filed = DateProperty(index=True)
    case_name = StringProperty()
    docket_id = IntegerProperty(default=None)
    docket_number = StringProperty(default=None)
    court_id = IntegerProperty(index=True)
    court_name = StringProperty(default=None)

    # Opinion type and voting data
    opinion_type = StringProperty(choices=OpinionType, default=None)
    scdb_votes_majority = IntegerProperty(default=None)
    scdb_votes_minority = IntegerProperty(default=None)

    # Original database IDs
    opinion_id = IntegerProperty(default=None)
    docket_db_id = IntegerProperty(default=None)
    court_db_id = IntegerProperty(default=None)

    # Enhanced metadata
    ai_summary = StringProperty(default=None)

    # Strategic composite indexes for common query patterns
    __indexes__ = {
        # Most common query pattern: finding opinions by court within a date range
        "court_date_idx": {"fields": ["court_id", "date_filed"], "type": "composite"},
    }

================
File: src/neo4j/neomodel_loader.py
================
#!/usr/bin/env python3
"""
Neomodel-based Loader for Legal Citation Network

A clean synchronous implementation using neomodel's native functionality for loading and managing
legal citation data in Neo4j. This loader handles both basic citation relationships
and enriched citation metadata.
"""

import logging
from datetime import datetime
from typing import List, Dict, Tuple, Optional, Any
import time
import os
from dotenv import load_dotenv
from neo4j import GraphDatabase

from neomodel import config, install_all_labels
from .models import Opinion
from src.llm_extraction.models import CombinedResolvedCitationAnalysis
import json

# Load environment variables
load_dotenv()

# Neo4j Configuration
NEO4J_URI = os.getenv("NEO4J_URI", "neo4j://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "courtlistener")
NEO4J_DATABASE = os.getenv("NEO4J_DATABASE", "courtlistener")

# Neo4j driver instance
_neo4j_driver = None

def get_neo4j_driver():
    """
    Get the Neo4j driver instance, creating it if it doesn't exist.
    
    Returns:
        GraphDatabase.driver: Neo4j driver
    """
    global _neo4j_driver
    if _neo4j_driver is None:
        _neo4j_driver = GraphDatabase.driver(
            NEO4J_URI, 
            auth=(NEO4J_USER, NEO4J_PASSWORD),
            database=NEO4J_DATABASE
        )
    return _neo4j_driver

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class NeomodelLoader:
    """
    Loader class for managing legal citation data using neomodel.
    Handles both basic citation relationships and enriched citation metadata.
    """

    def __init__(
        self,
        uri: str,
        username: str,
        password: str,
        database: str = "courtlistener",
        batch_size: int = 1000,
    ):
        """
        Initialize the loader with connection details and settings.

        Args:
            uri: Neo4j server URI
            username: Neo4j username
            password: Neo4j password
            database: Name of the Neo4j database to use
            batch_size: Default batch size for operations
        """
        self.uri = uri
        self.database = database
        self.batch_size = batch_size

        # Configure neomodel connection
        config.DATABASE_URL = f"bolt://{username}:{password}@{uri}/{database}"

        # Install constraints and indexes defined in the models
        self.setup_schema()

    def setup_schema(self) -> None:
        """
        Install all constraints and indexes defined in the models.
        This is handled automatically by neomodel based on model definitions.
        """
        try:
            install_all_labels()
            logger.info("Successfully installed schema constraints and indexes")
        except Exception as e:
            logger.error(f"Error setting up schema: {str(e)}")
            raise

    def create_or_update_opinion(self, data: Dict[str, Any]) -> Opinion:
        """
        Create a new opinion node or update an existing one while preserving existing metadata.

        Args:
            data: Dictionary containing opinion data

        Returns:
            Opinion node instance
        """
        if "cluster_id" not in data:
            raise ValueError("cluster_id is required for opinion creation/update")

        # Handle date conversion if needed
        if "date_filed" in data and isinstance(data["date_filed"], str):
            data["date_filed"] = datetime.strptime(
                data["date_filed"], "%Y-%m-%d"
            ).date()

        try:
            # First try to get existing opinion
            try:
                opinion = Opinion.nodes.get(cluster_id=data["cluster_id"])
                # Update only provided fields, preserving existing data
                for key, value in data.items():
                    if value is not None and key != "cluster_id":
                        setattr(opinion, key, value)
                opinion.save()
                logger.debug(f"Updated opinion with cluster_id: {opinion.cluster_id}")
                return opinion
            except Opinion.DoesNotExist:
                # Create new opinion if it doesn't exist
                opinion = Opinion.create(
                    {
                        "cluster_id": data["cluster_id"],
                        **{
                            k: v
                            for k, v in data.items()
                            if k != "cluster_id" and v is not None
                        },
                    }
                )
                logger.debug(
                    f"Created new opinion with cluster_id: {opinion.cluster_id}"
                )
                return opinion

        except Exception as e:
            logger.error(
                f"Error creating/updating opinion {data.get('cluster_id')}: {str(e)}"
            )
            raise

    def create_citation(
        self,
        citing_opinion: Opinion,
        cited_opinion: Opinion,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """
        Create or update a citation relationship between two opinions using get_or_create.
        If relationship exists, updates metadata and maintains version history.

        Args:
            citing_opinion: The opinion doing the citing
            cited_opinion: The opinion being cited
            metadata: Optional metadata for the citation relationship
        """
        try:
            # Ensure we have metadata dict and timestamp
            metadata = metadata or {}
            current_time = datetime.now()
            metadata.setdefault("timestamp", current_time)
            metadata.setdefault("version", 1)
            metadata.setdefault("other_metadata_versions", "[]")

            # Use get_or_create to atomically get or create the relationship
            rel = citing_opinion.cites.relationship(cited_opinion)
            if rel:
                # Relationship exists - update with new metadata while preserving history
                try:
                    other_metadata_versions = json.loads(rel.other_metadata_versions)
                except (json.JSONDecodeError, AttributeError, TypeError):
                    other_metadata_versions = []

                # Track changes for audit trail
                changes = []
                for key, new_value in metadata.items():
                    if new_value is not None and hasattr(rel, key):
                        old_value = getattr(rel, key)
                        if old_value != new_value:
                            changes.append(
                                {
                                    "field": key,
                                    "old_value": old_value,
                                    "updated_at": current_time.isoformat(),
                                    "version": rel.version,
                                }
                            )

                if changes:
                    other_metadata_versions.extend(changes)
                    metadata["other_metadata_versions"] = json.dumps(
                        other_metadata_versions
                    )
                    metadata["version"] = rel.version + 1

                # Update relationship with new metadata
                citing_opinion.cites.replace(cited_opinion, metadata)
                logger.debug(
                    f"Updated citation relationship: {citing_opinion.cluster_id}->"
                    f"{cited_opinion.cluster_id} with {len(changes)} field changes"
                )
            else:
                # Create new relationship
                citing_opinion.cites.connect(cited_opinion, metadata)
                logger.debug(
                    f"Created new citation relationship: {citing_opinion.cluster_id}->"
                    f"{cited_opinion.cluster_id}"
                )

            # Update citation count if available
            try:
                if hasattr(cited_opinion, "increment_citation_count"):
                    cited_opinion.increment_citation_count()
            except Exception as count_error:
                logger.warning(
                    f"Failed to increment citation count for opinion {cited_opinion.cluster_id}: {str(count_error)}"
                )

        except Exception as e:
            logger.error(
                f"Error creating/updating citation {citing_opinion.cluster_id}->"
                f"{cited_opinion.cluster_id}: {str(e)}"
            )
            raise

    def load_opinion_list(
        self, opinions_data: List[Dict[str, Any]], batch_size: Optional[int] = None
    ) -> List[Opinion]:
        """
        Load multiple opinions in batches using get_or_create's bulk creation capability.

        Args:
            opinions_data: List of opinion data dictionaries
            batch_size: Optional override for batch size

        Returns:
            List of created/updated Opinion instances
        """
        batch_size = batch_size or self.batch_size
        results = []
        failed_items = []

        for i in range(0, len(opinions_data), batch_size):
            batch = opinions_data[i : i + batch_size]
            try:
                # Convert date strings to datetime objects
                processed_batch = []
                for data in batch:
                    if "date_filed" in data and isinstance(data["date_filed"], str):
                        data["date_filed"] = datetime.strptime(
                            data["date_filed"], "%Y-%m-%d"
                        ).date()
                    # Filter out None values
                    properties = {k: v for k, v in data.items() if v is not None}
                    processed_batch.append(properties)

                # Bulk create/get all nodes in the batch
                nodes = Opinion.get_or_create(*processed_batch)
                results.extend(nodes)

                logger.info(
                    f"Processed batch of {len(nodes)} opinions (total: {len(results)}, failed: {len(failed_items)})"
                )
            except Exception as e:
                logger.error(f"Failed to process batch: {str(e)}")
                failed_items.extend(batch)

        if failed_items:
            logger.warning(
                f"Failed to process {len(failed_items)} opinions. Consider retrying these items separately."
            )
        return results

    def _process_citation_pair(
        self, citing_id: int, cited_id: int, metadata: Dict[str, Any]
    ) -> None:
        """
        Process a single citation pair.

        Args:
            citing_id: Cluster ID of the citing opinion
            cited_id: Cluster ID of the cited opinion
            metadata: Metadata for the citation relationship
        """
        try:
            # Use create_or_update_opinion for both opinions to ensure metadata preservation
            citing_opinion = self.create_or_update_opinion({"cluster_id": citing_id})
            cited_opinion = self.create_or_update_opinion({"cluster_id": cited_id})

            # Create the citation relationship
            self.create_citation(citing_opinion, cited_opinion, metadata)

        except Exception as e:
            logger.error(f"Error processing citation {citing_id}->{cited_id}: {str(e)}")
            raise

    def load_citation_pairs(
        self,
        citation_pairs: List[Tuple[int, int]],
        source: str,
        batch_size: Optional[int] = None,
    ) -> None:
        """
        Load basic citation relationships between opinions.

        Args:
            citation_pairs: List of (citing_id, cited_id) tuples
            source: Source identifier for the citations
            batch_size: Optional override for batch size
        """
        batch_size = batch_size or self.batch_size
        total_processed = 0

        for i in range(0, len(citation_pairs), batch_size):
            batch = citation_pairs[i : i + batch_size]

            for citing_id, cited_id in batch:
                try:
                    self._process_citation_pair(citing_id, cited_id, {"source": source})
                    total_processed += 1
                except Exception:
                    pass

            logger.info(f"Processed {total_processed} citation relationships")

    def load_enriched_citations(
        self, citations_data: List[CombinedResolvedCitationAnalysis], source: str
    ) -> None:
        """
        Load enriched citation data including metadata and AI-generated summaries.

        Args:
            citations_data: List of citation analysis objects
            source: Source identifier for the citations
        """
        for citation_analysis in citations_data:
            try:
                # Create or update the citing opinion with AI summary
                citing_opinion = self.create_or_update_opinion(
                    {
                        "cluster_id": citation_analysis.cluster_id,
                        "ai_summary": citation_analysis.brief_summary,
                        "date_filed": citation_analysis.date,
                    }
                )

                # Make this a dict with the section name as the key and the citations as the value
                sections = {
                    "majority": citation_analysis.majority_opinion_citations,
                    "concurrent": citation_analysis.concurrent_opinion_citations,
                    "dissent": citation_analysis.dissenting_citations,
                }

                for section_name, citations in sections.items():
                    for citation in citations:
                        if citation.resolved_opinion_cluster:
                            # Create or update the cited opinion
                            cited_opinion = self.create_or_update_opinion(
                                {
                                    "cluster_id": citation.resolved_opinion_cluster,
                                    "type": citation.type,
                                }
                            )

                            # Create the enriched citation relationship
                            metadata = {
                                "source": source,
                                "opinion_section": section_name,
                                "treatment": citation.treatment,
                                "relevance": citation.relevance,
                                "reasoning": citation.reasoning,
                                "citation_text": citation.citation_text,
                                "page_number": citation.page_number,
                            }

                            # Filter out None values
                            metadata = {
                                k: v for k, v in metadata.items() if v is not None
                            }

                            self.create_citation(
                                citing_opinion, cited_opinion, metadata
                            )

            except Exception as e:
                logger.error(
                    f"Error processing citations for opinion {citation_analysis.cluster_id}: {str(e)}"
                )
                continue


# Example usage (synchronous):
# def main():
#     NEO4J_URI = "localhost:7687"
#     NEO4J_USER = "neo4j"
#     NEO4J_PASSWORD = "courtlistener"
#     loader = NeomodelLoader(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)
#     opinions = [
#         {
#             "cluster_id": 1001,
#             "date_filed": "2020-01-01",
#             "case_name": "Smith v. Jones",
#             "court_id": 5,
#             "court_name": "Supreme Court",
#         }
#     ]
#     loader.load_opinions(opinions)
#     citation_pairs = [(1001, 1002), (1001, 1003)]
#     loader.load_citation_pairs(citation_pairs, source="courtlistener")
#
# if __name__ == "__main__":
#     main()

================
File: src/neo4j_import/helper_scripts/query_build_citation_map.sh
================
#!/bin/bash

# I have no idea why one is in python and the other is in bash
# This is a bash script that queries the PostgreSQL database for the citation map


# PostgreSQL connection details
DB_NAME="courtlistener"
DB_USER="courtlistener"  # default PostgreSQL user, change if needed
DB_HOST="localhost"
OUTPUT_FILE="cl_citation_map.csv"

psql --command "\COPY (
   SELECT 
    so.cluster_id as cited_cluster_id,
    so2.cluster_id as citing_cluster_id,
    'cl_citation_map' as source
FROM search_opinionscited cited
LEFT JOIN search_opinion so ON cited.cited_opinion_id = so.id
LEFT JOIN search_opinion so2 ON cited.citing_opinion_id = so2.id
LEFT JOIN search_opinioncluster soc1 ON so.cluster_id = soc1.id
LEFT JOIN search_opinioncluster soc2 ON so2.cluster_id = soc2.id
WHERE soc1.precedential_status = 'Published'
AND soc2.precedential_status = 'Published'
) TO '$OUTPUT_FILE' WITH (
    FORMAT csv,
    ENCODING utf8,
    FORCE_QUOTE *,
    HEADER
);" --host "$DB_HOST" --username "$DB_USER" --dbname "$DB_NAME"

================
File: src/neo4j_import/helper_scripts/query_build_opinion_cluster.py
================
#!/usr/bin/env python3

import psycopg2
import psycopg2.extras
import csv
from typing import Iterator
import logging
from datetime import datetime

# Set up logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Database connection parameters
DB_PARAMS = {
    "dbname": "courtlistener",
    "user": "courtlistener",
    "password": "postgrespassword",  # Add password matching the user
    "host": "localhost",
    "cursor_factory": psycopg2.extras.DictCursor,  # Returns results as dictionaries
}

# Output file configuration
OUTPUT_FILE = "neo4j_import/cl_opinion_cluster_nodes.csv"
BATCH_SIZE = 10000  # Number of records to process at once


def get_query() -> str:
    return """
        WITH first_opinions AS (
            SELECT DISTINCT ON (cluster_id) *
            FROM search_opinion
            ORDER BY cluster_id, 
                CASE 
                    WHEN type = '010combined' THEN 0
                    WHEN type = '020lead' THEN 1
                    ELSE 2
                END,
                id
        )
        SELECT  
            fo.cluster_id as cluster_id, 
            soc.date_filed as date_filed,
            soc.case_name as case_name,
            soc.scdb_votes_majority as scdb_votes_majority,
            soc.scdb_votes_minority as scdb_votes_minority,
            sd.case_name as docket_case_name,
            sd.docket_number as docket_number,
            sc.full_name as court_name,
            sd.id as docket_db_id,
            fo.id as opinion_db_id,
            fo.type as opinion_type,
            sd.court_id as court_db_id
        FROM first_opinions fo
        JOIN search_opinioncluster soc on fo.cluster_id = soc.id
        JOIN search_docket sd on soc.docket_id = sd.id
        JOIN search_court sc on sd.court_id = sc.id
        WHERE soc.precedential_status = 'Published'
    """


def fetch_records(cursor) -> Iterator[dict]:
    """Fetch records in batches using server-side cursor"""
    query = get_query()

    # Declare a named server-side cursor
    cursor.execute(f"DECLARE opinion_cursor CURSOR FOR {query}")

    while True:
        cursor.execute(f"FETCH {BATCH_SIZE} FROM opinion_cursor")
        records = cursor.fetchall()
        if not records:
            break
        for record in records:
            yield dict(record)


def write_csv(records: Iterator[dict], filename: str):
    """Write records to CSV file in Neo4j admin import format"""
    total_records = 0

    # Define fieldnames for Neo4j admin import format
    fieldnames = [
        ":LABEL",
        "cluster_id:ID",
        "soc_date_filed:DATE",
        "case_name:STRING",
        "soc_scdb_votes_majority:INT",
        "soc_scdb_votes_minority:INT",
        "docket_case_name:STRING",
        "docket_number:STRING",
        "court_name:STRING",
        "_search_docket_id:INT",
        "_search_opinion_id:INT",
        "_search_opinion_type:STRING",
        "_court_id:STRING",
    ]

    with open(filename, "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        for record in records:
            # Transform record to Neo4j admin format
            neo4j_record = {
                ":LABEL": "Opinion",
                "cluster_id:ID": record["cluster_id"],
                "soc_date_filed:DATE": record["date_filed"],
                "case_name:STRING": record["case_name"],
                "soc_scdb_votes_majority:INT": (
                    record["scdb_votes_majority"]
                    if record["scdb_votes_majority"]
                    else ""
                ),
                "soc_scdb_votes_minority:INT": (
                    record["scdb_votes_minority"]
                    if record["scdb_votes_minority"]
                    else ""
                ),
                "docket_case_name:STRING": record["docket_case_name"],
                "docket_number:STRING": record["docket_number"],
                "court_name:STRING": record["court_name"],
                "_search_docket_id:INT": record["docket_db_id"],
                "_search_opinion_id:INT": record["opinion_db_id"],
                "_search_opinion_type:STRING": record["opinion_type"],
                "_court_id:STRING": record["court_db_id"],
            }
            writer.writerow(neo4j_record)
            total_records += 1

            if total_records % BATCH_SIZE == 0:
                logger.info(f"Processed {total_records} records")

    logger.info(f"Completed processing {total_records} total records")


def main():
    logger.info("Starting opinion cluster export")
    conn = None
    cursor = None

    try:
        # Connect to the database
        conn = psycopg2.connect(**DB_PARAMS)
        cursor = conn.cursor()

        # Process and write records
        records = fetch_records(cursor)
        write_csv(records, OUTPUT_FILE)

        logger.info(f"Successfully exported data to {OUTPUT_FILE}")

    except Exception as e:
        logger.error(f"Error during export: {str(e)}")
        raise

    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()
        logger.info("Database connection closed")


if __name__ == "__main__":
    main()

================
File: src/neo4j_import/citations.header
================
cited_cluster_id:START_ID,citing_cluster_id:END_ID,source:STRING

================
File: src/neo4j_import/import_opinions.sh
================
#!/bin/bash

#############
#
#
#      NOTE THIS FILE MUST BE WITHIN neo4j_import/ in order to get mounted
#      TO THE DOCKER CONTAINER
# 
#
#########

# Set password from environment variable
export NEO4J_PASSWORD=courtlistener # TODO Hardcoded variable

# Create timestamp for the bad entries log
TIMESTAMP=$(date +%Y-%m-%d_%H-%M-%S)

# Run the import with correct syntax
/var/lib/neo4j/bin/neo4j-admin database import full neo4j \
    --nodes=/var/lib/neo4j/import/opinions.header,/var/lib/neo4j/import/cl_opinion_cluster_nodes.csv \
    --relationships=/var/lib/neo4j/import/citations.header,/var/lib/neo4j/import/cl_citation_map_source.csv \
    --overwrite-destination \
    --verbose \
    --multiline-fields=true \
    --bad-tolerance=90000000 \
    --skip-bad-relationships=true \
    --report-file=/var/lib/neo4j/import/import_report_${TIMESTAMP}.log

# Create indexes (after database is running)
/var/lib/neo4j/bin/cypher-shell -u neo4j -p $NEO4J_PASSWORD "
CREATE CONSTRAINT opinion_cluster_id IF NOT EXISTS
FOR (o:Opinion) REQUIRE o.cluster_id IS UNIQUE;

CREATE INDEX opinion_date_filed IF NOT EXISTS
FOR (o:Opinion) ON (o.soc_date_filed);

CREATE INDEX opinion_court_id IF NOT EXISTS
FOR (o:Opinion) ON (o._court_id);

CREATE RANGE INDEX opinion_cluster_id_range IF NOT EXISTS
FOR (o:Opinion) ON (o.cluster_id);
"

echo "Import completed! Created indexes."

# Get count of imported opinions and relationships
echo "Counting imported opinions and citations..."
/var/lib/neo4j/bin/cypher-shell -u neo4j -p $NEO4J_PASSWORD "
MATCH (o:Opinion) 
RETURN count(o) as opinion_count;

MATCH ()-[r]->() 
RETURN type(r) as relationship_type, count(r) as relationship_count;"

================
File: src/neo4j_import/opinions.header
================
Opinion:LABEL,cluster_id:ID,soc_date_filed:DATE,case_name:STRING,soc_scdb_votes_majority:INT,soc_scdb_votes_minority:INT,docket_case_name:STRING,docket_number:STRING,court_name:STRING,_search_docket_id:INT,_search_opinion_id:INT,_search_opinion_type:STRING,_court_id:STRING
:IGNORE_FIRST_LINE

================
File: src/postgres/database.py
================
"""
PostgreSQL database connection and session management.

This module provides the core database functionality for the application,
including connection management, session creation, and utility functions.
"""
import os
import logging
from typing import Generator, Optional
from contextlib import contextmanager

from sqlalchemy import create_engine, Engine, Column, Integer, String, and_, text
from sqlalchemy.orm import sessionmaker, Session, declarative_base
from sqlalchemy.pool import QueuePool
from sqlalchemy.exc import SQLAlchemyError
from dotenv import load_dotenv

# Import eyecite for citation parsing
try:
    from eyecite import get_citations
    from eyecite.resolve import resolve_citations
    EYECITE_AVAILABLE = True
except ImportError:
    EYECITE_AVAILABLE = False

# Configure logger
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Get database credentials from environment variables
DB_USER = os.getenv("DB_USER", "courtlistener")
DB_PASSWORD = os.getenv("DB_PASSWORD", "postgrespassword")
DB_HOST = os.getenv("DB_HOST", "localhost")
DB_PORT = os.getenv("DB_PORT", "5432")
DB_NAME = os.getenv("DB_NAME", "courtlistener")

# Create database URL
DATABASE_URL = f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"

# Create engine with connection pooling
engine = create_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=10,
    max_overflow=10,
    pool_timeout=30,
    pool_pre_ping=True,
)

# Create session factory
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# Create base class for declarative models
Base = declarative_base()

# Legacy name for backward compatibility
Session = SessionLocal

def get_engine() -> Engine:
    """
    Get the SQLAlchemy engine instance.
    
    Returns:
        Engine: SQLAlchemy engine
    """
    return engine

def get_session_factory(engine_instance=None):
    """
    Get the SQLAlchemy session factory.
    
    Args:
        engine_instance: Optional engine instance to bind to the session factory
        
    Returns:
        sessionmaker: SQLAlchemy session factory
    """
    if engine_instance:
        return sessionmaker(autocommit=False, autoflush=False, bind=engine_instance)
    return SessionLocal

def get_db() -> Generator[Session, None, None]:
    """
    Get a database session.

    Yields:
        Session: A SQLAlchemy session
    """
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

@contextmanager
def get_db_session():
    """
    Context manager to handle database sessions.

    Yields:
        Session: A SQLAlchemy session
    """
    session = SessionLocal()
    try:
        yield session
        session.commit()
    except Exception:
        session.rollback()
        raise
    finally:
        session.close()

def init_db():
    """
    Initialize the database by creating all tables.

    Note: In production, use alembic for migrations instead.
    """
    # Import models here to avoid circular imports
    from .models import Base
    Base.metadata.create_all(bind=engine)

def verify_connection() -> bool:
    """
    Verify database connection.

    Returns:
        bool: True if connection is successful, False otherwise
    """
    try:
        with get_db_session() as session:
            result = session.execute(text("SELECT version()"))
            version = result.scalar()
            logger.info(f"Successfully connected to PostgreSQL. Version: {version}")
            return True
    except Exception as e:
        logger.error(f"Failed to connect to database: {str(e)}")
        return False

# Citation lookup utility class and functions
class Citation(Base):
    """Model representing citation records in the database."""
    __tablename__ = "search_citation"

    id = Column(Integer, primary_key=True)
    volume = Column(String)
    reporter = Column(String)
    page = Column(String)
    type = Column(Integer)
    cluster_id = Column(Integer)

    def __repr__(self):
        return f"<Citation(volume={self.volume}, reporter={self.reporter}, page={self.page})>"

def find_cluster_id(citation_string: str) -> Optional[int]:
    """
    Find cluster_id for a given citation string using eyecite for parsing.
    
    Args:
        citation_string: Citation text to look up
        
    Returns:
        Optional[int]: Cluster ID if found, None otherwise
    """
    if not citation_string or not EYECITE_AVAILABLE:
        return None

    try:
        # Extract citations using eyecite
        citations = get_citations(citation_string)
        if not citations:
            logger.debug(f"No citations found in: {citation_string}")
            return None

        # Resolve the citations
        resolved_citations = resolve_citations(citations)
        if not resolved_citations:
            logger.debug(f"Could not resolve citations in: {citation_string}")
            return None

        # Use the first resolved citation
        resolved = list(resolved_citations.keys())[0]
        resolved_citation = resolved.citation

        # Extract normalized components
        volume = str(resolved_citation.groups["volume"])
        reporter = resolved_citation.groups["reporter"]
        page = str(resolved_citation.groups["page"])

        if not volume or not reporter or not page:
            logger.warning(
                f"Invalid citation lookup: {citation_string}, missing volume, reporter, or page"
            )
            return None

        # Use database session with proper error handling
        with get_db_session() as session:
            # Query the database with resolved components
            result = (
                session.query(Citation)
                .filter(
                    and_(
                        Citation.volume == volume,
                        Citation.reporter == reporter,
                        Citation.page == page,
                    )
                )
                .first()
            )

            return result.cluster_id if result else None

    except Exception as e:
        logger.error(f"Error processing citation '{citation_string}': {str(e)}")
        return None

================
File: src/postgres/models.py
================
from datetime import datetime
from enum import Enum

from sqlalchemy import Column, Integer, String, DateTime, ForeignKey, Enum as SQLAEnum, Text, Boolean
from sqlalchemy.orm import declarative_base, relationship

from src.llm_extraction.models import CitationType, CitationTreatment, OpinionSection, OpinionType

Base = declarative_base()

"""
NOTE THESE ARE NOT YET APPLIED TO OUR DATABASE AS THIS IS THE LAST THING TO BE DONE.

I BELIEVE WE CAN EVEN BUILD THIS FROM THE NEO4J OUTPUT
"""
class OpinionClusterExtraction(Base):
    __tablename__ = "opinion_cluster_extraction"

    id = Column(Integer, primary_key=True, autoincrement=True)
    cluster_id = Column(
        Integer,
        unique=True,
        index=True,
        nullable=False,
        comment="The unique identifier for this opinion cluster",
    )
    date_filed = Column(
        DateTime, index=True, nullable=False, comment="The date the opinion was filed"
    )
    brief_summary = Column(
        String, nullable=False, comment="3-5 sentences describing the core holding"
    )
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    updated_at = Column(
        DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False
    )

    citations = relationship("CitationExtraction", back_populates="opinion_cluster")
   #  opinion_text = relationship("OpinionText", back_populates="opinion_cluster", uselist=False)




class CitationExtraction(Base):
    __tablename__ = "citation_extraction"

    id = Column(Integer, primary_key=True, autoincrement=True)
    opinion_cluster_extraction_id = Column(
        Integer, ForeignKey("opinion_cluster_extraction.id"), index=True, nullable=False
    )

    # Citation section and type
    section = Column(SQLAEnum(OpinionSection), nullable=False)
    citation_type = Column(
        SQLAEnum(CitationType),
        nullable=False,
        comment="The type of legal document being cited",
    )

    # Citation metadata
    citation_text = Column(
        String, nullable=False, comment="The entirety of the extracted citation text"
    )
    page_number = Column(
        Integer, nullable=True, comment="The page number where this citation appears"
    )
    treatment = Column(
        SQLAEnum(CitationTreatment),
        nullable=True,
        comment="How this citation is treated (POSITIVE, NEGATIVE, etc)",
    )
    relevance = Column(Integer, nullable=True, comment="Relevance score between 1-4")
    reasoning = Column(
        String, nullable=True, comment="Detailed analysis explaining citation context"
    )

    # TODO, add in keys for other resolutions when we get there, i.e. for codes/laws/regulations.
    resolved_opinion_cluster = Column(
        Integer,
        index=True,
        nullable=True,
        comment="The cluster_id of the resolved citation for case law",
    )
    resolved_text = Column(String, nullable=True, comment="The resolved citation text")

    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    updated_at = Column(
        DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False
    )

    # Relationship
    opinion_cluster = relationship(
        "OpinionClusterExtraction", back_populates="citations"
    )

================
File: .dockerignore
================
# Git
.git
.gitignore

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
*.egg-info/
.installed.cfg
*.egg
.pytest_cache/

# Virtual environments
.env
.venv
venv/
ENV/

# Docker
.dockerignore
docker-compose.yml
Dockerfile

# IDE files
.idea/
.vscode/
*.swp
*.swo

# Project specific
neo4j_logs/
neo4j_data/
neo4j_config/
neo4j_plugins/
postgres_data/

================
File: .env.prod
================
# Production Environment Configuration

# Application Settings
ENVIRONMENT=production
LOG_LEVEL=INFO
API_PORT=8000
WORKERS=4

# API Security
CORS_ORIGINS=https://your-production-domain.com,https://admin.your-production-domain.com
API_KEY=change_me_to_secure_key_in_production

# Database Configuration (PostgreSQL)
POSTGRES_USER=courtlistener
POSTGRES_PASSWORD=change_this_to_a_secure_password
POSTGRES_DB=courtlistener
DB_USER=courtlistener
DB_PASSWORD=change_this_to_a_secure_password
DB_HOST=postgres
DB_PORT=5432
DB_NAME=courtlistener
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=30
DB_POOL_RECYCLE=1800

# Neo4j Configuration
NEO4J_AUTH=neo4j/change_this_to_a_secure_password
NEO4J_URI=neo4j://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=change_this_to_a_secure_password
NEO4J_DATABASE=courtlistener
NEO4J_BOLT_CONNECTION_TIMEOUT=20
NEO4J_DRIVER_MAX_CONNECTION_LIFETIME=3600
NEO4J_DRIVER_MAX_CONNECTION_POOL_SIZE=100

================
File: .gitignore
================
# Python-generated files
__pycache__/
*.py[oc]
build/
dist/
wheels/
*.egg-info

# Virtual environments
.venv

*.json
*.csv
*.log
.env

================
File: .python-version
================
3.11

================
File: cline_planning.md
================
# OS Legal Explorer - Implementation Plan

## Project Overview

The OS Legal Explorer is a web application designed to visualize and explore legal citation networks. The project aims to:

1. Process court opinions from CourtListener's database
2. Extract citations and their metadata using LLMs (Gemini)
3. Build a graph database of citation relationships in Neo4j
4. Create an interactive web application to visualize and explore these relationships

## Current State Assessment

The project already has several key components implemented:

### Data Models
- Pydantic models for citation analysis (`src/llm_extraction/models.py`)
- SQLAlchemy models for database storage (`src/postgres/models.py`)
- Neo4j node/relationship models (`src/neo4j/models.py`)

### Data Processing
- LLM-based citation extraction using Gemini (`src/llm_extraction/rate_limited_gemini.py`)
- Citation resolution utilities (`src/postgres/db_utils.py`)
- Neo4j data loading utilities (`src/neo4j/neomodel_loader.py`, `src/neo4j/legacy_load_neo4j.py`)

### Infrastructure
- Docker setup for Neo4j (`docker-compose.yml`)
- Import scripts for Neo4j (`src/neo4j_import/`)

## Implementation Plan

### Phase 1: Complete the Data Pipeline

#### 1.1 Unified Pipeline Architecture
- Create a modular pipeline that connects:
  - PostgreSQL opinion data extraction
  - LLM-based citation analysis
  - Citation resolution
  - Neo4j graph database loading

#### 1.2 Pipeline Components
1. **Opinion Extractor**
   - Extract opinions from PostgreSQL in batches
   - Filter by court, date, or other criteria
   - Output: Dataframe of opinions with text and metadata

2. **Citation Analyzer**
   - Process opinions through Gemini LLM
   - Extract citations with treatment, relevance, and reasoning
   - Output: `CitationAnalysis` objects

3. **Citation Resolver**
   - Resolve citation strings to opinion cluster IDs
   - Link citations to their source opinions
   - Output: `CombinedResolvedCitationAnalysis` objects

4. **Graph Loader**
   - Load resolved citations into Neo4j
   - Create opinion nodes and citation relationships
   - Store metadata on relationships

#### 1.3 Pipeline Orchestration
- Create a unified CLI for running the pipeline
- Implement logging and error handling
- Add checkpointing for resumable processing
- Provide configuration options for each component

### Phase 2: Web Application Development

#### 2.1 Backend Architecture
- FastAPI application structure
- API endpoints for:
  - Opinion retrieval
  - Citation network queries
  - Graph visualization data

#### 2.2 Database Integration
- Neo4j query service for citation networks
- PostgreSQL service for opinion text and metadata
- Caching layer for performance optimization

#### 2.3 Frontend Development
- HTMX-based interactive UI
- D3.js visualizations:
  - Citation network graphs
  - Citation timeline views
  - Court/judge influence maps
- Tailwind CSS for styling

#### 2.4 Key Features
- Citation network exploration
- Opinion text viewer with highlighted citations
- Citation treatment analysis
- Court/judge influence visualization
- Search and filtering capabilities

### Phase 3: Deployment and Optimization

#### 3.1 Performance Optimization
- Query optimization for Neo4j
- Caching strategies
- Pagination and lazy loading

#### 3.2 Deployment Architecture
- Docker containerization
- Use Caddy
- Database backup and restore procedures

#### 3.3 Monitoring and Maintenance
- Logging and error tracking
- Performance monitoring
- Database maintenance scripts

## Technical Architecture

```mermaid
graph TD
    A[PostgreSQL Database] -->|Extract Opinions| B[Opinion Processor]
    B -->|Process Text| C[Gemini LLM]
    C -->|Citation Analysis| D[Citation Resolver]
    D -->|Resolved Citations| E[Neo4j Loader]
    E -->|Graph Data| F[Neo4j Database]
    
    F -->|Query API| G[FastAPI Backend]
    G -->|JSON Data| H[HTMX Frontend]
    H -->|Visualization| I[D3.js Graphs]
    
    J[User] -->|Interacts| H
```

## Data Flow

```mermaid
sequenceDiagram
    participant PG as PostgreSQL
    participant LLM as Gemini LLM
    participant NJ as Neo4j
    participant API as FastAPI
    participant UI as Frontend

    PG->>LLM: Opinion Text
    LLM->>PG: Citation Analysis
    PG->>NJ: Resolved Citations
    UI->>API: User Request
    API->>NJ: Graph Query
    NJ->>API: Citation Network
    API->>UI: Visualization Data
```

## Implementation Timeline



## Next Steps

1. Finalize the pipeline architecture
2. Implement the missing components
3. Create a unified CLI for the pipeline
4. Begin backend development
5. Design the frontend interface

## Detailed Implementation Tasks

### Pipeline Integration Tasks

1. **Create Pipeline Using Python**
   - create an endpoint in the fastapi model only from requests from localhost (not public) that allows loading of a csv file e2e via local pathway.
      - this includes LLM Citation Extraction, using some of the methods in gemini_rate_limited.py
      - then resolving the cluster_id using db_utils (already done within pydantic model, but probably should refactor)
      - then adding these data to the neo4j database.
      - then adding to the postgres database. 
      - ideal worker friendly so we can do 15 RPM for google's limit.

5. **Optimize Neo4j Loading**
   - Batch processing for efficient graph updates
   - Transaction management for data consistency
   - Index optimization for query performance

### Web Application Tasks

1. **FastAPI Backend Setup**
   - Project structure and dependency management
   - No accounts or login, just a web page with a few tabs.
   - API documentation with Swagger/ReDoc

2. **Core API Endpoints**
   - `/api/opinions` - Opinion retrieval and search
   - `/api/citations` - Citation network queries
   - `/api/stats` - Network statistics and metrics

3. **Frontend Structure**
   - HTMX setup with Alpine.js for interactivity
   - Component-based architecture
   - Responsive design with Tailwind CSS

4. **Visualization Components**
   - Force-directed graph for citation networks
   - Timeline visualization for citation evolution
   - Heatmap for court/judge influence

5. **User Interface Features**
   - Search and filtering interface
   - Opinion text viewer with citation highlighting
   - Citation details panel
   - Network exploration controls

## Technical Challenges and Solutions


### Challenge 4: Interactive Visualization of Large Networks
- **Solution**: Implement progressive loading of graph data
- **Solution**: Use clustering and filtering to reduce visual complexity
- **Solution**: Optimize D3.js rendering with other tools if needed. 

## Future Enhancements

1. - Create dataset of each state's legal codes and import to postgres so we can  resolve the large majority of the rest of the citation types (assuming)


### LATER ENHANCEMENTS
1. **Advanced Analytics**
   - Citation influence scoring
   - Topic modeling of opinions
   - Prediction of future citations

2. **Integration with Other Legal Databases**
   - Westlaw/Lexis integration
   - International court databases
   - Legislative and regulatory databases

3. **Collaboration Features**
   - Annotation and commenting
   - Sharing of network views
   - Collaborative research tools

================
File: docker-compose.yml
================
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DB_USER=courtlistener
      - DB_PASSWORD=postgrespassword
      - DB_HOST=host.docker.internal
      - DB_PORT=5432
      - DB_NAME=courtlistener
      - NEO4J_URI=neo4j://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=courtlistener
      - NEO4J_DATABASE=courtlistener
      - PYTHONPATH=/app
    volumes:
      - .:/app
    depends_on:
      - neo4j
    restart: always
    extra_hosts:
      - "host.docker.internal:host-gateway"

  neo4j:
    image: neo4j:latest
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - ./src/neo4j_import:/var/lib/neo4j/import
    environment:
      - NEO4J_AUTH=neo4j/courtlistener
    ports:
      - "7474:7474"
      - "7687:7687"
    restart: always

volumes:
  neo4j_data:
  neo4j_logs:

================
File: docker-entrypoint.sh
================
#!/bin/bash
set -e

# Basic check for PostgreSQL
echo "Checking connection to PostgreSQL..."
pg_isready -h ${DB_HOST} -p ${DB_PORT} -U ${DB_USER} || echo "PostgreSQL not yet accessible - API will retry connections as needed"

# Basic check for Neo4j
echo "Checking connection to Neo4j..."
curl -s --head "http://${NEO4J_USER}:${NEO4J_PASSWORD}@neo4j:7474/browser/" > /dev/null || echo "Neo4j not yet accessible - API will retry connections as needed"

# Start the application
echo "Starting the application..."
exec "$@"

================
File: Dockerfile
================
FROM python:3.11-slim

WORKDIR /app

# Install only essential system dependencies
RUN apt-get update && apt-get install -y \
    postgresql-client \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Mount the code as a volume instead of copying
# (we'll mount the code from the host in docker-compose)

# Install project in development mode
COPY pyproject.toml .
RUN pip install -e .

# Make entrypoint script executable
COPY docker-entrypoint.sh .
RUN chmod +x /app/docker-entrypoint.sh

# Basic environment settings
ENV PYTHONPATH=/app
ENV PORT=8000
# Don't write .pyc files (cleaner)
ENV PYTHONDONTWRITEBYTECODE=1
# Don't buffer output (better logging)
ENV PYTHONUNBUFFERED=1

# Expose port
EXPOSE ${PORT}

# Use entrypoint script
ENTRYPOINT ["/app/docker-entrypoint.sh"]

# Simple development command
CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

================
File: pyproject.toml
================
[project]
name = "os-legal-explorer"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "dotenv>=0.9.9",
    "fastapi>=0.115.8",
    "google-genai>=1.3.0",
    "json-repair>=0.39.1",
    "neo4j>=5.27.0",
    "neomodel>=5.4.3",
    "pydantic>=2.10.6",
    "sqlalchemy>=2.0.38",
    "tqdm>=4.67.1",
    "uvicorn>=0.34.0",
]

================
File: query.sh
================
psql --command "\COPY (
    select  
	so.cluster_id as cluster_id, 
	so.type as so_type, 
	so.id as so_id, 
	so.page_count as so_page_count, 
	so.plain_text as so_plain_text, 
	so.html as so_html, 
	so.html_with_citations as so_html_with_citations, 
	so.xml_harvard as so_xml_harvard,
	so.html_columbia as so_html_columbia,
	so.html_anon_2020 as so_html_anon_2020,
	so.html_lawbox as so_html_lawbox,
	soc.case_name as cluster_case_name,
	soc.citation_count as soc_citation_count,
	soc.nature_of_suit as soc_nature_of_suit,
	soc.scdb_decision_direction as soc_scdb_decision_direction,
	soc.scdb_votes_majority as soc_scdb_votes_majority,
	soc.scdb_votes_minority as soc_scdb_votes_minority,
	soc.date_filed as soc_date_filed,
	sd.nature_of_suit as docker_nature_of_suit,
	sd.case_name as docker_case_name,
	sd.id,
	sd.docket_number as sd_docket_number,
 	sd.docket_number_core as sd_docket_number_Core
	from search_opinion so 
	left join search_opinioncluster soc on so.cluster_id = soc.id
	left join search_docket sd on soc.docket_id = sd.id 
where 
	sd.court_id = 'scotus'
	and soc.precedential_status = 'Published'
) TO 'query_with_date.csv' WITH (
    FORMAT csv,
    ENCODING utf8,
    ESCAPE '\\',
    FORCE_QUOTE *,
    HEADER
); " --host "localhost" --username "courtlistener" --dbname "courtlistener"

================
File: README.md
================
# Goal of this project
I want to create a very interactive website using d3.js, htmx, and a python backend preferrably something simple, like fastapi. 

- There will be no accounts, just a frontend with some different views, probably tabs work for now.
- We will get more into the visualizations later.


I have a local postgres database running with a copy `[courtlistenerof](http://courtlistener.com)'s dumps of all of their data. With specifically the Court Opinions text, I will be mappings of citations found in these cases to one another, while staying consistent with courtlistener's data to hopefully integrate one day. I've built a pipeline already to use LLM's to extract the citation's treatment and other metadata out of all citations in the opinion's text. I also have a neo4j database where I'd like to build a graph database of these citation representations. I have models built out for all three of these datatypes to use in python. I have a VPS with 64GB of RAM and 2TB SSD, 16 dedicated server cores, so we have plenty of processing power.

Needs to be done:
- connect the pipes and start getting citations loaded in! 
- build out a python web backend to manage two things:
    1. the pipeline between the csv export of the database table, calling LLM, resolving opinion cluster ID, and entering into neo4j.
    2. web app using a python + htmx + d3.js + tailwindcss to create a nice looking website to display our rich graphs of information.
    

## Docker Setup

This project is dockerized for easy deployment with Neo4j and the API service. It assumes PostgreSQL is running separately on your host machine.

### Prerequisites

- Docker and Docker Compose installed on your system
- PostgreSQL running on your host machine
- PostgreSQL database created for the project

### PostgreSQL Setup

Before starting the Docker services, make sure your PostgreSQL server:
1. Is running and accessible
2. Has a database named `courtlistener` (or update the DB_NAME in docker-compose.yml)
3. Has a user `courtlistener` with password `postgrespassword` (or update DB_USER and DB_PASSWORD in docker-compose.yml)
4. Allows connections from Docker containers (check pg_hba.conf)

### Quick Start

1. Clone the repository
2. Configure environment variables in docker-compose.yml to match your PostgreSQL setup
3. Start the services:

```bash
docker-compose up -d
```

This will start the following services:
- API service on port 8000
- Neo4j database on ports 7474 (HTTP) and 7687 (Bolt)

### Accessing Services

- FastAPI documentation: http://localhost:8000/docs
- Neo4j Browser: http://localhost:7474/browser/

### Development

For development, the code is mounted as a volume, so changes will be reflected immediately.
When you make changes to the API code, the server will auto-reload.

### Stopping Services

```bash
docker-compose down
```

To remove all data (volumes):

```bash
docker-compose down -v
```

## Simple Deployment Guide

This project can be easily deployed on a VPS with Docker. Here's a straightforward approach:

### Prerequisites

- A VPS with Docker and Docker Compose installed
- PostgreSQL installed and running on the VPS or accessible from the VPS
- Git installed
- At least 2GB RAM recommended

### PostgreSQL Setup on VPS

If PostgreSQL is running on the same VPS:

1. Install PostgreSQL if not already installed:
```bash
sudo apt update
sudo apt install postgresql postgresql-contrib
```

2. Create a database and user:
```bash
sudo -u postgres psql
postgres=# CREATE DATABASE courtlistener;
postgres=# CREATE USER courtlistener WITH PASSWORD 'postgrespassword';
postgres=# GRANT ALL PRIVILEGES ON DATABASE courtlistener TO courtlistener;
postgres=# \q
```

3. Update PostgreSQL configuration to allow connections:
```bash
sudo nano /etc/postgresql/*/main/pg_hba.conf
```
Add this line (adjust for your network setup):
```
host    all             all             172.17.0.0/16           md5
```
Then restart PostgreSQL:
```bash
sudo systemctl restart postgresql
```

### Deployment Steps

1. Clone the repository on your VPS:
```bash
git clone <your-repo-url>
cd os-legal-explorer
```

2. Update docker-compose.yml to connect to your PostgreSQL server:
```yaml
# In the api service section:
environment:
  - DB_HOST=your-postgres-host-or-ip
  - DB_PORT=5432
  - DB_USER=courtlistener
  - DB_PASSWORD=your-secure-password
  - DB_NAME=courtlistener
  # ... other environment variables ...
```

3. Make the entrypoint script executable:
```bash
chmod +x docker-entrypoint.sh
```

4. Start the services:
```bash
docker-compose up -d
```

This will start:
- The API service on port 8000
- Neo4j database on ports 7474 (HTTP) and 7687 (Bolt)

### Importing Data

If you have CourtListener data:

1. For PostgreSQL data (since PostgreSQL is running separately):
```bash
# For SQL dumps (run directly on your PostgreSQL server)
psql -U courtlistener -d courtlistener -f /path/to/your/dump.sql

# For CSV files (adjust paths as needed)
# Example using psql's \copy command
psql -U courtlistener -d courtlistener
courtlistener=# \copy table_name FROM '/path/to/your/data.csv' WITH CSV HEADER;
```

2. Use the API pipeline endpoints to process data:
- Access the API docs at http://your-vps-ip:8000/docs
- Use the pipeline endpoints to extract, process and load data

### Monitoring and Maintenance

- View logs with `docker-compose logs -f`
- Restart services with `docker-compose restart`
- Stop all services with `docker-compose down`



================================================================
End of Codebase
================================================================
